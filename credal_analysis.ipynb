{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emSGhu3Dg37_"
      },
      "outputs": [],
      "source": [
        "# Credal Set Analysis for Uncertainty Quantification in NLG\n",
        "# Implementation for analyzing uncertainty in open-ended text generation\n",
        "\n",
        "# %% [markdown]\n",
        "# # Credal Set Analysis for Uncertainty Quantification in Open-Ended Text Generation\n",
        "#\n",
        "# This notebook implements credal set analysis with comprehensive visualizations\n",
        "# for the paper: \"Disentangling Aleatoric and Epistemic Uncertainty in Open-Ended Text Generation\"\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Setup and Imports\n",
        "\n",
        "# %%\n",
        "print(\"Installing required packages...\")\n",
        "\n",
        "# Core packages\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q pandas numpy matplotlib seaborn scipy\n",
        "!pip install -q scikit-learn plotly\n",
        "!pip install -q sentence-transformers umap-learn\n",
        "!pip install -q shapely  # For geometric operations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.spatial import ConvexHull, Delaunay\n",
        "from scipy.spatial.distance import cdist, directed_hausdorff\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from typing import List, Dict, Tuple, Optional, Set\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "from itertools import combinations\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# For embeddings and analysis\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap\n",
        "\n",
        "print(\"Package import complete\")\n",
        "\n",
        "# Set publication-quality style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 11\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 13\n",
        "plt.rcParams['xtick.labelsize'] = 10\n",
        "plt.rcParams['ytick.labelsize'] = 10\n",
        "plt.rcParams['legend.fontsize'] = 10\n",
        "plt.rcParams['figure.titlesize'] = 14\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "# Define color palette for publication\n",
        "COLORS = {\n",
        "    'human': '#2E86AB',      # Deep blue\n",
        "    'base': '#A23B72',        # Burgundy\n",
        "    'instruct': '#F18F01',    # Orange\n",
        "    'primary': '#2E86AB',\n",
        "    'secondary': '#A23B72',\n",
        "    'tertiary': '#F18F01',\n",
        "    'quaternary': '#C73E1D',\n",
        "    'accent': '#6A994E'\n",
        "}\n",
        "\n",
        "# Model-specific colors\n",
        "MODEL_COLORS = {\n",
        "    'human': '#2E86AB',\n",
        "    'GPT2-XL': '#A23B72',\n",
        "    'Gemma-2B': '#F18F01',\n",
        "    'Mistral-7B-Instruct': '#C73E1D',\n",
        "    'Llama-3.1-8B-Instruct': '#6A994E'\n",
        "}\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Configure Paths and Load Data\n",
        "\n",
        "# %%\n",
        "# Mount storage (modify as needed for your environment)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define paths - modify these for your setup\n",
        "BASE_DIR = '/content/drive/MyDrive/analysis_data'  # Change to your directory\n",
        "SAVE_DIR = os.path.join(BASE_DIR, 'uq_analysis')\n",
        "RESULTS_DIR = os.path.join(SAVE_DIR, 'results')\n",
        "FIGURES_DIR = os.path.join(SAVE_DIR, 'figures')\n",
        "CREDAL_DIR = os.path.join(SAVE_DIR, 'credal_analysis')\n",
        "\n",
        "# Create credal analysis directory\n",
        "os.makedirs(CREDAL_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Working directory: {SAVE_DIR}\")\n",
        "print(f\"Loading results from: {RESULTS_DIR}\")\n",
        "\n",
        "# Load the main results\n",
        "try:\n",
        "    # Load uncertainty analysis\n",
        "    uncertainty_df = pd.read_csv(os.path.join(RESULTS_DIR, 'uncertainty_analysis.csv'))\n",
        "    print(f\"Loaded uncertainty analysis: {uncertainty_df.shape}\")\n",
        "\n",
        "    # Load comprehensive metrics\n",
        "    metrics_df = pd.read_csv(os.path.join(RESULTS_DIR, 'comprehensive_metrics.csv'))\n",
        "    print(f\"Loaded comprehensive metrics: {metrics_df.shape}\")\n",
        "\n",
        "    # Load all stories\n",
        "    stories_df = pd.read_parquet(os.path.join(RESULTS_DIR, 'all_stories_complete.parquet'))\n",
        "    print(f\"Loaded stories dataset: {stories_df.shape}\")\n",
        "\n",
        "    # Load selected prompts metadata\n",
        "    with open(os.path.join(SAVE_DIR, 'selected_prompts_verified.json'), 'r') as f:\n",
        "        selected_prompts = json.load(f)\n",
        "    print(f\"Loaded {len(selected_prompts)} prompts metadata\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    raise\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Pre-compute Embeddings\n",
        "\n",
        "# %%\n",
        "print(\"Pre-computing embeddings...\")\n",
        "\n",
        "# Check if embeddings are already cached\n",
        "embeddings_cache_path = os.path.join(CREDAL_DIR, 'embeddings_cache.pkl')\n",
        "\n",
        "if os.path.exists(embeddings_cache_path):\n",
        "    print(\"Loading cached embeddings...\")\n",
        "    with open(embeddings_cache_path, 'rb') as f:\n",
        "        story_embeddings = pickle.load(f)\n",
        "    print(f\"Loaded {len(story_embeddings)} cached embeddings\")\n",
        "else:\n",
        "    print(\"Computing embeddings...\")\n",
        "    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Get unique stories and compute embeddings\n",
        "    unique_stories = stories_df['story'].unique()\n",
        "    print(f\"Computing embeddings for {len(unique_stories)} unique stories...\")\n",
        "\n",
        "    # Batch encode for efficiency\n",
        "    batch_size = 128\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(unique_stories), batch_size), desc=\"Encoding batches\"):\n",
        "        batch = unique_stories[i:i+batch_size]\n",
        "        batch_embeddings = sentence_model.encode(batch, show_progress_bar=False)\n",
        "        all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "    # Create dictionary mapping\n",
        "    story_embeddings = {story: emb for story, emb in zip(unique_stories, all_embeddings)}\n",
        "\n",
        "    # Cache for future use\n",
        "    with open(embeddings_cache_path, 'wb') as f:\n",
        "        pickle.dump(story_embeddings, f)\n",
        "    print(f\"Computed and cached {len(story_embeddings)} embeddings\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Credal Set Framework\n",
        "\n",
        "# %%\n",
        "class CredalSetAnalyzer:\n",
        "    \"\"\"\n",
        "    Credal set framework with fast computation and uncertainty decomposition.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, story_embeddings: Dict, seed=42):\n",
        "        \"\"\"\n",
        "        Initialize credal set analyzer.\n",
        "\n",
        "        Args:\n",
        "            story_embeddings: Pre-computed embeddings dictionary\n",
        "            seed: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "        self.story_embeddings = story_embeddings\n",
        "        self._cache = {}  # Cache for repeated computations\n",
        "\n",
        "    def compute_diversity_metrics(self, stories: List[str]) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Compute diversity metrics using pre-computed embeddings.\n",
        "        \"\"\"\n",
        "        if len(stories) < 2:\n",
        "            return {\n",
        "                'semantic': np.array([0.0, 0.0, 0.0]),\n",
        "                'lexical': np.array([0.0, 0.0, 0.0]),\n",
        "                'syntactic': np.array([0.0, 0.0, 0.0]),\n",
        "                'combined': np.zeros(9)\n",
        "            }\n",
        "\n",
        "        diversity_metrics = {}\n",
        "\n",
        "        # 1. Semantic Diversity\n",
        "        embeddings = []\n",
        "        for story in stories:\n",
        "            if story in self.story_embeddings:\n",
        "                embeddings.append(self.story_embeddings[story])\n",
        "\n",
        "        if len(embeddings) >= 2:\n",
        "            embeddings = np.array(embeddings)\n",
        "\n",
        "            # Fast pairwise cosine similarity\n",
        "            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "            normalized = embeddings / norms\n",
        "            cos_sim_matrix = np.dot(normalized, normalized.T)\n",
        "\n",
        "            # Extract upper triangle\n",
        "            mask = np.triu(np.ones_like(cos_sim_matrix), k=1).astype(bool)\n",
        "            semantic_distances = 1 - cos_sim_matrix[mask]\n",
        "\n",
        "            diversity_metrics['semantic'] = np.array([\n",
        "                np.mean(semantic_distances),\n",
        "                np.std(semantic_distances),\n",
        "                np.percentile(semantic_distances, 75) - np.percentile(semantic_distances, 25)\n",
        "            ])\n",
        "        else:\n",
        "            diversity_metrics['semantic'] = np.zeros(3)\n",
        "\n",
        "        # 2. Lexical Diversity\n",
        "        vocabs = [set(story.lower().split()) for story in stories]\n",
        "\n",
        "        all_tokens = ' '.join(stories).lower().split()\n",
        "        ttr = len(set(all_tokens)) / len(all_tokens) if all_tokens else 0\n",
        "\n",
        "        jaccard_distances = []\n",
        "        for i, j in combinations(range(len(vocabs)), 2):\n",
        "            intersection = len(vocabs[i] & vocabs[j])\n",
        "            union = len(vocabs[i] | vocabs[j])\n",
        "            if union > 0:\n",
        "                jaccard_distances.append(1 - intersection / union)\n",
        "\n",
        "        if jaccard_distances:\n",
        "            diversity_metrics['lexical'] = np.array([\n",
        "                ttr,\n",
        "                np.mean(jaccard_distances),\n",
        "                np.std(jaccard_distances)\n",
        "            ])\n",
        "        else:\n",
        "            diversity_metrics['lexical'] = np.array([ttr, 0, 0])\n",
        "\n",
        "        # 3. Syntactic Diversity\n",
        "        all_lengths = []\n",
        "        for story in stories:\n",
        "            lengths = [len(s.split()) for s in story.split('.') if s.strip()]\n",
        "            all_lengths.extend(lengths)\n",
        "\n",
        "        if all_lengths:\n",
        "            diversity_metrics['syntactic'] = np.array([\n",
        "                np.std(all_lengths),\n",
        "                np.percentile(all_lengths, 75) - np.percentile(all_lengths, 25),\n",
        "                max(all_lengths) - min(all_lengths) if all_lengths else 0\n",
        "            ])\n",
        "        else:\n",
        "            diversity_metrics['syntactic'] = np.zeros(3)\n",
        "\n",
        "        # 4. Combined\n",
        "        diversity_metrics['combined'] = np.concatenate([\n",
        "            diversity_metrics['semantic'],\n",
        "            diversity_metrics['lexical'],\n",
        "            diversity_metrics['syntactic']\n",
        "        ])\n",
        "\n",
        "        return diversity_metrics\n",
        "\n",
        "\n",
        "    def construct_credal_set(self, diversity_vectors: List[np.ndarray],\n",
        "                            min_points: int = 4) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Construct credal set with pre-computed statistics.\n",
        "        \"\"\"\n",
        "        if len(diversity_vectors) < min_points:\n",
        "            return None\n",
        "\n",
        "        points = np.array(diversity_vectors)\n",
        "\n",
        "        # Standardize\n",
        "        scaler = StandardScaler()\n",
        "        points_scaled = scaler.fit_transform(points)\n",
        "\n",
        "        try:\n",
        "            # Add minimal noise for numerical stability\n",
        "            noise = np.random.normal(0, 1e-8, points_scaled.shape)\n",
        "            points_noisy = points_scaled + noise\n",
        "\n",
        "            hull = ConvexHull(points_noisy)\n",
        "\n",
        "            # Store necessary information\n",
        "            result = {\n",
        "                'hull': hull,\n",
        "                'points_scaled': points_scaled,\n",
        "                'points_original': points,\n",
        "                'centroid_scaled': np.mean(points_scaled, axis=0),\n",
        "                'centroid_original': np.mean(points, axis=0),\n",
        "                'vertices_scaled': points_scaled[hull.vertices],\n",
        "                'vertices_indices': hull.vertices,\n",
        "                'volume': hull.volume,\n",
        "                'scaler': scaler,\n",
        "                'n_points': len(points)\n",
        "            }\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not compute convex hull: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def compute_calibration_metrics(self, credal1: Dict, credal2: Dict) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute calibration metrics between two credal sets.\n",
        "        \"\"\"\n",
        "        if credal1 is None or credal2 is None:\n",
        "            return {\n",
        "                'overlap': 0.0,\n",
        "                'centroid_dist': float('inf'),\n",
        "                'hausdorff': float('inf'),\n",
        "                'volume_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        # Use cached computation if available\n",
        "        cache_key = (id(credal1), id(credal2))\n",
        "        if cache_key in self._cache:\n",
        "            return self._cache[cache_key]\n",
        "\n",
        "        # Centroid distance (normalized)\n",
        "        c1 = credal1['centroid_original']\n",
        "        c2 = credal2['centroid_original']\n",
        "        scale = np.mean([np.std(credal1['points_original']),\n",
        "                         np.std(credal2['points_original'])])\n",
        "        metrics['centroid_dist'] = np.linalg.norm(c1 - c2) / scale if scale > 0 else 0\n",
        "\n",
        "        # Overlap approximation\n",
        "        v1 = credal1['vertices_scaled'][:min(50, len(credal1['vertices_scaled']))]\n",
        "        v2 = credal2['vertices_scaled'][:min(50, len(credal2['vertices_scaled']))]\n",
        "\n",
        "\n",
        "        if len(v1) > 0 and len(v2) > 0:\n",
        "            min_dists = np.min(cdist(v1, v2), axis=1)\n",
        "            metrics['overlap'] = np.mean(min_dists < 1.0)\n",
        "        else:\n",
        "            metrics['overlap'] = 0.0\n",
        "\n",
        "        # Hausdorff distance\n",
        "        if len(v1) > 0 and len(v2) > 0:\n",
        "            metrics['hausdorff'] = max(\n",
        "                np.max(np.min(cdist(v1, v2), axis=1)),\n",
        "                np.max(np.min(cdist(v2, v1), axis=1))\n",
        "            )\n",
        "        else:\n",
        "            metrics['hausdorff'] = float('inf')\n",
        "\n",
        "\n",
        "        # Volume ratio\n",
        "        metrics['volume_ratio'] = credal1['volume'] / credal2['volume'] if credal2['volume'] > 0 else 0\n",
        "\n",
        "        # Cache result\n",
        "        self._cache[cache_key] = metrics\n",
        "\n",
        "        return metrics\n",
        "\n",
        "\n",
        "    def decompose_uncertainty(self, model_name: str,\n",
        "                              credal_sets: Dict,\n",
        "                              human_credal: Dict = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Decompose uncertainty into epistemic and aleatoric components.\n",
        "        \"\"\"\n",
        "        model_sources = [s for s in credal_sets.keys() if s.startswith(model_name)]\n",
        "\n",
        "        if len(model_sources) < 2:\n",
        "            return {\n",
        "                'epistemic_uncertainty': 0.0,\n",
        "                'aleatoric_uncertainty': 0.0,\n",
        "                'total_uncertainty': 0.0,\n",
        "                'epistemic_ratio': 0.0,\n",
        "                'n_strategies': len(model_sources)\n",
        "            }\n",
        "\n",
        "        div_type = 'combined'\n",
        "\n",
        "        # Collect credal data\n",
        "        credal_data_list = []\n",
        "        for source in model_sources:\n",
        "            if source in credal_sets and div_type in credal_sets[source]:\n",
        "                credal_data_list.append(credal_sets[source][div_type])\n",
        "\n",
        "        valid_credal = [c for c in credal_data_list if c is not None]\n",
        "\n",
        "        if len(valid_credal) < 2:\n",
        "            return {\n",
        "                'epistemic_uncertainty': 0.0,\n",
        "                'aleatoric_uncertainty': 0.0,\n",
        "                'total_uncertainty': 0.0,\n",
        "                'epistemic_ratio': 0.0,\n",
        "                'n_strategies': len(model_sources)\n",
        "            }\n",
        "\n",
        "        # Combine all points for proper scaling\n",
        "        all_original_points = []\n",
        "        for c in valid_credal:\n",
        "            all_original_points.append(c['points_original'])\n",
        "\n",
        "        combined_points = np.vstack(all_original_points)\n",
        "        common_scaler = StandardScaler()\n",
        "        all_scaled = common_scaler.fit_transform(combined_points)\n",
        "\n",
        "        # Compute centroids for each strategy\n",
        "        centroids_scaled = []\n",
        "        start_idx = 0\n",
        "        for original in all_original_points:\n",
        "            end_idx = start_idx + len(original)\n",
        "            centroid = np.mean(all_scaled[start_idx:end_idx], axis=0)\n",
        "            centroids_scaled.append(centroid)\n",
        "            start_idx = end_idx\n",
        "\n",
        "\n",
        "        # Epistemic: Variance across strategy centroids\n",
        "        centroids_array = np.array(centroids_scaled)\n",
        "        epistemic_uncertainty = np.mean(np.var(centroids_array, axis=0))\n",
        "\n",
        "        # Aleatoric: Average within-strategy variance\n",
        "        within_variances = []\n",
        "        start_idx = 0\n",
        "        for original in all_original_points:\n",
        "            end_idx = start_idx + len(original)\n",
        "            strategy_points = all_scaled[start_idx:end_idx]\n",
        "            within_var = np.mean(np.var(strategy_points, axis=0))\n",
        "            within_variances.append(within_var)\n",
        "            start_idx = end_idx\n",
        "\n",
        "        aleatoric_uncertainty = np.mean(within_variances)\n",
        "\n",
        "        # Total uncertainty\n",
        "        total_uncertainty = epistemic_uncertainty + aleatoric_uncertainty\n",
        "\n",
        "        # Epistemic ratio\n",
        "        epistemic_ratio = epistemic_uncertainty / total_uncertainty if total_uncertainty > 0 else 0\n",
        "\n",
        "        result = {\n",
        "            'epistemic_uncertainty': epistemic_uncertainty,\n",
        "            'aleatoric_uncertainty': aleatoric_uncertainty,\n",
        "            'total_uncertainty': total_uncertainty,\n",
        "            'epistemic_ratio': epistemic_ratio,\n",
        "            'n_strategies': len(model_sources),\n",
        "            'centroid_dispersion': np.std(centroids_array),\n",
        "            'mean_within_variance': aleatoric_uncertainty\n",
        "        }\n",
        "\n",
        "        # If human baseline provided, compute relative metrics\n",
        "        if human_credal is not None and div_type in human_credal:\n",
        "            human_points = human_credal[div_type]['points_original']\n",
        "            human_variance = np.mean(np.var(human_points, axis=0))\n",
        "            result['aleatoric_ratio_to_human'] = aleatoric_uncertainty / human_variance if human_variance > 0 else 0\n",
        "\n",
        "\n",
        "        return result\n",
        "\n",
        "# Initialize analyzer\n",
        "credal_analyzer = CredalSetAnalyzer(story_embeddings)\n",
        "print(\"Credal Set Analyzer initialized\")\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Build Credal Sets\n",
        "\n",
        "# %%\n",
        "print(\"\\nBuilding credal sets...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Dictionary to store credal sets and diversity vectors\n",
        "credal_sets = {}\n",
        "diversity_vectors = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "# Get unique sources\n",
        "sources = stories_df['source'].unique()\n",
        "print(f\"Found {len(sources)} sources to analyze\")\n",
        "\n",
        "# Process each source\n",
        "for source in tqdm(sources, desc=\"Building credal sets\"):\n",
        "\n",
        "    # Process each prompt\n",
        "    for prompt_id in selected_prompts.keys():\n",
        "        # Get stories for this prompt and source\n",
        "        prompt_stories = stories_df[\n",
        "            (stories_df['prompt_id'] == prompt_id) &\n",
        "            (stories_df['source'] == source)\n",
        "        ]['story'].tolist()\n",
        "\n",
        "        if len(prompt_stories) >= 5:\n",
        "            # Compute diversity vectors\n",
        "            div_vectors = credal_analyzer.compute_diversity_metrics(prompt_stories[:10])\n",
        "\n",
        "            # Store vectors\n",
        "            for div_type in ['semantic', 'lexical', 'syntactic', 'combined']:\n",
        "                diversity_vectors[source][div_type].append(div_vectors[div_type])\n",
        "\n",
        "    # Construct credal sets\n",
        "    credal_sets[source] = {}\n",
        "    for div_type in ['semantic', 'lexical', 'syntactic', 'combined']:\n",
        "        if diversity_vectors[source][div_type]:\n",
        "            credal_data = credal_analyzer.construct_credal_set(\n",
        "                diversity_vectors[source][div_type]\n",
        "            )\n",
        "            credal_sets[source][div_type] = credal_data\n",
        "\n",
        "            if credal_data is not None:\n",
        "                print(f\"  {source} - {div_type}: {credal_data['n_points']} points, \"\n",
        "                      f\"volume: {credal_data['volume']:.4f}\")\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nBuilt all credal sets in {elapsed:.1f} seconds\")\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Compute Calibration Metrics\n",
        "\n",
        "# %%\n",
        "print(\"\\nComputing calibration metrics...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Human baseline\n",
        "human_credal_sets = credal_sets.get('human', {})\n",
        "human_vectors = diversity_vectors.get('human', {})\n",
        "\n",
        "calibration_results = []\n",
        "\n",
        "# Process all sources\n",
        "for source in tqdm(credal_sets.keys(), desc=\"Computing calibration\"):\n",
        "    if source == 'human':\n",
        "        continue\n",
        "\n",
        "    # Parse model and strategy\n",
        "    if '_' in source:\n",
        "        parts = source.split('_')\n",
        "        model = parts[0]\n",
        "        if 'temperature' in source:\n",
        "            strategy = 'temperature'\n",
        "            value = parts[-1]\n",
        "        elif 'top_p' in source:\n",
        "            strategy = 'top_p'\n",
        "            value = parts[-1]\n",
        "        elif 'top_k' in source:\n",
        "            strategy = 'top_k'\n",
        "            value = parts[-1]\n",
        "        elif 'typical' in source:\n",
        "            strategy = 'typical_p'\n",
        "            value = parts[-1]\n",
        "        else:\n",
        "            strategy = '_'.join(parts[1:])\n",
        "            value = ''\n",
        "    else:\n",
        "        model = source\n",
        "        strategy = 'default'\n",
        "        value = ''\n",
        "\n",
        "    # Initialize metrics\n",
        "    cal_metrics = {\n",
        "        'source': source,\n",
        "        'model': model,\n",
        "        'strategy': strategy,\n",
        "        'strategy_value': value\n",
        "    }\n",
        "\n",
        "    # Compute calibration for each diversity type\n",
        "    for div_type in ['semantic', 'lexical', 'syntactic', 'combined']:\n",
        "        if div_type in credal_sets[source] and div_type in human_credal_sets:\n",
        "            model_credal = credal_sets[source][div_type]\n",
        "            human_credal = human_credal_sets[div_type]\n",
        "\n",
        "            if model_credal is not None and human_credal is not None:\n",
        "                metrics = credal_analyzer.compute_calibration_metrics(\n",
        "                    model_credal, human_credal\n",
        "                )\n",
        "\n",
        "                cal_metrics[f'{div_type}_calibration'] = metrics['overlap']\n",
        "                cal_metrics[f'{div_type}_centroid_dist'] = metrics['centroid_dist']\n",
        "                cal_metrics[f'{div_type}_hausdorff'] = metrics['hausdorff']\n",
        "                cal_metrics[f'{div_type}_volume_ratio'] = metrics['volume_ratio']\n",
        "\n",
        "\n",
        "    # Overall metrics\n",
        "    cal_scores = []\n",
        "    dist_scores = []\n",
        "    for dt in ['semantic', 'lexical', 'syntactic']:\n",
        "        if f'{dt}_calibration' in cal_metrics:\n",
        "            cal_scores.append(cal_metrics[f'{dt}_calibration'])\n",
        "        if f'{dt}_centroid_dist' in cal_metrics:\n",
        "            dist_scores.append(cal_metrics[f'{dt}_centroid_dist'])\n",
        "\n",
        "    cal_metrics['overall_calibration'] = np.mean(cal_scores) if cal_scores else 0.0\n",
        "    cal_metrics['overall_distance'] = np.mean(dist_scores) if dist_scores else float('inf')\n",
        "\n",
        "    calibration_results.append(cal_metrics)\n",
        "\n",
        "# Convert to DataFrame\n",
        "calibration_df = pd.DataFrame(calibration_results)\n",
        "calibration_df = calibration_df.sort_values('overall_calibration', ascending=False)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nCalibration computed in {elapsed:.1f} seconds\")\n",
        "\n",
        "print(\"\\nBest calibrated configurations:\")\n",
        "print(calibration_df[['source', 'overall_calibration', 'overall_distance']].head(10).round(3))\n",
        "\n",
        "# Save results\n",
        "calibration_df.to_csv(os.path.join(CREDAL_DIR, 'calibration_metrics.csv'), index=False)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Uncertainty Decomposition (Epistemic vs Aleatoric)\n",
        "\n",
        "# %%\n",
        "print(\"\\nComputing uncertainty decomposition...\")\n",
        "\n",
        "uncertainty_decomp = []\n",
        "\n",
        "# Get human baseline for aleatoric reference\n",
        "human_credal = credal_sets.get('human', {})\n",
        "\n",
        "for model_name in ['GPT2-XL', 'Gemma-2B', 'Mistral-7B-Instruct', 'Llama-3.1-8B-Instruct']:\n",
        "    decomp = credal_analyzer.decompose_uncertainty(\n",
        "        model_name, credal_sets, human_credal\n",
        "    )\n",
        "    decomp['model'] = model_name\n",
        "    uncertainty_decomp.append(decomp)\n",
        "\n",
        "uncertainty_decomp_df = pd.DataFrame(uncertainty_decomp)\n",
        "\n",
        "print(\"\\nUncertainty Decomposition (Epistemic vs Aleatoric):\")\n",
        "print(uncertainty_decomp_df[['model', 'epistemic_uncertainty', 'aleatoric_uncertainty',\n",
        "                             'total_uncertainty', 'epistemic_ratio']].round(3))\n",
        "\n",
        "uncertainty_decomp_df.to_csv(os.path.join(CREDAL_DIR, 'uncertainty_decomposition.csv'), index=False)\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Strategy and Model Analysis\n",
        "\n",
        "# %%\n",
        "print(\"\\nAnalyzing best strategies and models...\")\n",
        "\n",
        "# 1. Best strategy per model\n",
        "print(\"\\n=== Best Decoding Strategy per Model ===\")\n",
        "best_strategy_per_model = []\n",
        "\n",
        "for model in ['GPT2-XL', 'Gemma-2B', 'Mistral-7B-Instruct', 'Llama-3.1-8B-Instruct']:\n",
        "    model_df = calibration_df[calibration_df['model'] == model]\n",
        "    if len(model_df) > 0:\n",
        "        best = model_df.iloc[0]\n",
        "        best_strategy_per_model.append({\n",
        "            'Model': model,\n",
        "            'Best Strategy': f\"{best['strategy']} {best['strategy_value']}\".strip(),\n",
        "            'Calibration': f\"{best['overall_calibration']:.3f}\",\n",
        "            'Distance': f\"{best['overall_distance']:.3f}\"\n",
        "        })\n",
        "\n",
        "strategy_table = pd.DataFrame(best_strategy_per_model)\n",
        "print(strategy_table.to_string(index=False))\n",
        "strategy_table.to_csv(os.path.join(CREDAL_DIR, 'best_strategy_per_model.csv'), index=False)\n",
        "\n",
        "# 2. Best model per strategy\n",
        "print(\"\\n=== Best Model per Decoding Strategy ===\")\n",
        "best_model_per_strategy = []\n",
        "\n",
        "for strategy in calibration_df['strategy'].unique():\n",
        "    strategy_df = calibration_df[calibration_df['strategy'] == strategy]\n",
        "    if len(strategy_df) > 0:\n",
        "        best = strategy_df.iloc[0]\n",
        "        best_model_per_strategy.append({\n",
        "            'Strategy': strategy,\n",
        "            'Best Model': best['model'],\n",
        "            'Calibration': f\"{best['overall_calibration']:.3f}\",\n",
        "            'Distance': f\"{best['overall_distance']:.3f}\"\n",
        "        })\n",
        "\n",
        "model_table = pd.DataFrame(best_model_per_strategy)\n",
        "print(model_table.to_string(index=False))\n",
        "model_table.to_csv(os.path.join(CREDAL_DIR, 'best_model_per_strategy.csv'), index=False)\n",
        "\n",
        "\n",
        "# 3. Strategy effectiveness across all models\n",
        "print(\"\\n=== Average Strategy Performance ===\")\n",
        "strategy_performance = calibration_df.groupby('strategy').agg({\n",
        "    'overall_calibration': ['mean', 'std'],\n",
        "    'overall_distance': ['mean', 'std']\n",
        "}).round(3)\n",
        "print(strategy_performance)\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Visualization 1: Credal Sets in PCA Space\n",
        "\n",
        "# %%\n",
        "print(\"\\nCreating credal set visualization...\")\n",
        "\n",
        "# Select sources for visualization\n",
        "viz_sources = ['human']\n",
        "for model in ['GPT2-XL', 'Mistral-7B-Instruct']:\n",
        "    model_sources = [s for s in credal_sets.keys() if s.startswith(model)]\n",
        "    if model_sources:\n",
        "        # Get best calibrated for this model\n",
        "        model_df = calibration_df[calibration_df['model'] == model]\n",
        "        if len(model_df) > 0:\n",
        "            viz_sources.append(model_df.iloc[0]['source'])\n",
        "\n",
        "print(f\"Visualizing: {viz_sources}\")\n",
        "\n",
        "# Collect combined diversity vectors\n",
        "all_vectors = []\n",
        "all_labels = []\n",
        "\n",
        "for source in viz_sources:\n",
        "    if source in diversity_vectors and 'combined' in diversity_vectors[source]:\n",
        "        vectors = diversity_vectors[source]['combined']\n",
        "        all_vectors.extend(vectors)\n",
        "        all_labels.extend([source] * len(vectors))\n",
        "\n",
        "\n",
        "if all_vectors:\n",
        "    # PCA for visualization\n",
        "    all_vectors_array = np.array(all_vectors)\n",
        "    pca = PCA(n_components=min(3, all_vectors_array.shape[1]), random_state=42)\n",
        "    vectors_pca = pca.fit_transform(all_vectors_array)\n",
        "\n",
        "    # Create figure\n",
        "    fig = plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Three 2D projections\n",
        "    if vectors_pca.shape[1] >= 3:\n",
        "        projections = [(0, 1, '1-2'), (0, 2, '1-3'), (1, 2, '2-3')]\n",
        "    else:\n",
        "        projections = [(0, 1, '1-2')]\n",
        "\n",
        "\n",
        "    for idx, proj_data in enumerate(projections):\n",
        "        ax = plt.subplot(1, len(projections), idx + 1)\n",
        "\n",
        "        pc1, pc2, label = proj_data if len(proj_data) == 3 else (0, 1, '1-2')\n",
        "\n",
        "        for i, source in enumerate(viz_sources):\n",
        "            source_mask = np.array(all_labels) == source\n",
        "            if np.any(source_mask):\n",
        "                source_points = vectors_pca[source_mask][:, [pc1, pc2]]\n",
        "\n",
        "                # Determine color\n",
        "                if source == 'human':\n",
        "                    color = COLORS['human']\n",
        "                    label_text = 'Human'\n",
        "                elif 'GPT2' in source:\n",
        "                    color = MODEL_COLORS['GPT2-XL']\n",
        "                    label_text = 'GPT2-XL'\n",
        "                else:\n",
        "                    color = MODEL_COLORS['Mistral-7B-Instruct']\n",
        "                    label_text = 'Mistral-7B'\n",
        "\n",
        "\n",
        "                # Plot points\n",
        "                ax.scatter(source_points[:, 0], source_points[:, 1],\n",
        "                          alpha=0.6, s=60, c=color,\n",
        "                          label=label_text,\n",
        "                          edgecolors='white', linewidth=0.5)\n",
        "\n",
        "                # Add convex hull\n",
        "                if len(source_points) >= 3:\n",
        "                    try:\n",
        "                        hull = ConvexHull(source_points)\n",
        "                        for simplex in hull.simplices:\n",
        "                            ax.plot(source_points[simplex, 0],\n",
        "                                   source_points[simplex, 1],\n",
        "                                   color=color, alpha=0.3, linewidth=2)\n",
        "                        ax.fill(source_points[hull.vertices, 0],\n",
        "                               source_points[hull.vertices, 1],\n",
        "                               color=color, alpha=0.1)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "\n",
        "        ax.set_xlabel(f'PC{pc1+1} ({pca.explained_variance_ratio_[pc1]:.1%})')\n",
        "        ax.set_ylabel(f'PC{pc2+1} ({pca.explained_variance_ratio_[pc2]:.1%})')\n",
        "        ax.set_title(f'Principal Components {pc1+1}-{pc2+1}')\n",
        "        ax.grid(True, alpha=0.3, linewidth=0.5)\n",
        "        ax.legend(frameon=True, fancybox=True, shadow=True)\n",
        "\n",
        "\n",
        "    plt.suptitle('Credal Sets in Principal Component Space', fontsize=14, y=1.02)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig_path = os.path.join(CREDAL_DIR, 'credal_sets_pca.png')\n",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved credal set visualization\")\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Visualization 2: Uncertainty Decomposition\n",
        "\n",
        "# %%\n",
        "print(\"\\nCreating uncertainty decomposition visualization...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "# Plot 1: Stacked bar chart of uncertainty components\n",
        "ax1 = axes[0, 0]\n",
        "models = uncertainty_decomp_df['model'].tolist()\n",
        "epistemic = uncertainty_decomp_df['epistemic_uncertainty'].tolist()\n",
        "aleatoric = uncertainty_decomp_df['aleatoric_uncertainty'].tolist()\n",
        "\n",
        "x_pos = np.arange(len(models))\n",
        "width = 0.6\n",
        "\n",
        "bars1 = ax1.bar(x_pos, aleatoric, width, label='Aleatoric',\n",
        "                color=COLORS['primary'], alpha=0.7)\n",
        "bars2 = ax1.bar(x_pos, epistemic, width, bottom=aleatoric,\n",
        "                label='Epistemic', color=COLORS['secondary'], alpha=0.7)\n",
        "\n",
        "ax1.set_xlabel('Model', fontsize=11)\n",
        "ax1.set_ylabel('Uncertainty', fontsize=11)\n",
        "ax1.set_title('Uncertainty Decomposition', fontsize=12)\n",
        "ax1.set_xticks(x_pos)\n",
        "ax1.set_xticklabels([m.replace('-', '\\n') for m in models], fontsize=9)\n",
        "ax1.legend(frameon=True, fancybox=True)\n",
        "ax1.grid(axis='y', alpha=0.3, linewidth=0.5)\n",
        "\n",
        "# Add value labels\n",
        "for i, (e, a) in enumerate(zip(epistemic, aleatoric)):\n",
        "    ax1.text(i, a/2, f'{a:.2f}', ha='center', va='center', fontsize=9, color='white')\n",
        "    ax1.text(i, a + e/2, f'{e:.2f}', ha='center', va='center', fontsize=9, color='white')\n",
        "\n",
        "\n",
        "# Plot 2: Epistemic ratio\n",
        "ax2 = axes[0, 1]\n",
        "epistemic_ratios = uncertainty_decomp_df['epistemic_ratio'].tolist()\n",
        "\n",
        "bars = ax2.bar(x_pos, epistemic_ratios, color=COLORS['tertiary'], alpha=0.7)\n",
        "ax2.set_xlabel('Model', fontsize=11)\n",
        "ax2.set_ylabel('Epistemic Ratio', fontsize=11)\n",
        "ax2.set_title('Proportion of Epistemic Uncertainty', fontsize=12)\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels([m.replace('-', '\\n') for m in models], fontsize=9)\n",
        "ax2.grid(axis='y', alpha=0.3, linewidth=0.5)\n",
        "ax2.set_ylim([0, 1])\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, epistemic_ratios):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "            f'{val:.2%}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "\n",
        "# Plot 3: Calibration vs Model Size\n",
        "ax3 = axes[1, 0]\n",
        "model_sizes = {\n",
        "    'GPT2-XL': 1.5,\n",
        "    'Gemma-2B': 2.0,\n",
        "    'Mistral-7B-Instruct': 7.0,\n",
        "    'Llama-3.1-8B-Instruct': 8.0\n",
        "}\n",
        "\n",
        "# Get best calibration per model\n",
        "best_cal_by_model = calibration_df.groupby('model')['overall_calibration'].max()\n",
        "\n",
        "sizes = []\n",
        "calibrations = []\n",
        "colors_list = []\n",
        "for model, size in model_sizes.items():\n",
        "    if model in best_cal_by_model.index:\n",
        "        sizes.append(size)\n",
        "        calibrations.append(best_cal_by_model[model])\n",
        "        colors_list.append(MODEL_COLORS[model])\n",
        "\n",
        "\n",
        "scatter = ax3.scatter(sizes, calibrations, s=200, alpha=0.7, c=colors_list, edgecolors='white', linewidth=2)\n",
        "\n",
        "# Add trend line\n",
        "z = np.polyfit(sizes, calibrations, 1)\n",
        "p = np.poly1d(z)\n",
        "ax3.plot(sizes, p(sizes), \"--\", alpha=0.5, color='gray')\n",
        "\n",
        "for i, model in enumerate(model_sizes.keys()):\n",
        "    if model in best_cal_by_model.index:\n",
        "        ax3.annotate(model.split('-')[0],\n",
        "                    (model_sizes[model], best_cal_by_model[model]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "\n",
        "ax3.set_xlabel('Model Size (Billions)', fontsize=11)\n",
        "ax3.set_ylabel('Best Calibration Score', fontsize=11)\n",
        "ax3.set_title('Calibration vs Model Size', fontsize=12)\n",
        "ax3.grid(True, alpha=0.3, linewidth=0.5)\n",
        "\n",
        "\n",
        "# Plot 4: Strategy performance heatmap\n",
        "ax4 = axes[1, 1]\n",
        "\n",
        "# Create strategy performance matrix\n",
        "strategies = calibration_df['strategy'].unique()\n",
        "models_list = ['GPT2-XL', 'Gemma-2B', 'Mistral-7B-Instruct', 'Llama-3.1-8B-Instruct']\n",
        "\n",
        "strategy_matrix = np.zeros((len(strategies), len(models_list)))\n",
        "for i in range(len(strategies)):\n",
        "    for j in range(len(models_list)):\n",
        "        mask = (calibration_df['strategy'] == strategies[i]) & (calibration_df['model'] == models_list[j])\n",
        "        if mask.any():\n",
        "            strategy_matrix[i, j] = calibration_df.loc[mask, 'overall_calibration'].values[0]\n",
        "        else:\n",
        "            strategy_matrix[i, j] = np.nan\n",
        "\n",
        "\n",
        "im = ax4.imshow(strategy_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=0.6)\n",
        "ax4.set_xticks(np.arange(len(models_list)))\n",
        "ax4.set_yticks(np.arange(len(strategies)))\n",
        "ax4.set_xticklabels([m.split('-')[0] for m in models_list], fontsize=9)\n",
        "ax4.set_yticklabels(strategies, fontsize=9)\n",
        "ax4.set_xlabel('Model', fontsize=11)\n",
        "ax4.set_ylabel('Strategy', fontsize=11)\n",
        "ax4.set_title('Strategy Performance Across Models', fontsize=12)\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(im, ax=ax4)\n",
        "cbar.set_label('Calibration', fontsize=10)\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(len(strategies)):\n",
        "    for j in range(len(models_list)):\n",
        "        if not np.isnan(strategy_matrix[i, j]):\n",
        "            text = ax4.text(j, i, f'{strategy_matrix[i, j]:.2f}',\n",
        "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
        "\n",
        "\n",
        "plt.suptitle('Uncertainty Analysis and Model Performance', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_path = os.path.join(CREDAL_DIR, 'uncertainty_analysis.png')\n",
        "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved uncertainty analysis visualization\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 11. Visualization 3: Epistemic vs Aleatoric Trade-off\n",
        "\n",
        "# %%\n",
        "print(\"\\nCreating epistemic-aleatoric trade-off visualization...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot 1: Scatter plot of epistemic vs aleatoric\n",
        "ax1 = axes[0]\n",
        "\n",
        "for i, row in uncertainty_decomp_df.iterrows():\n",
        "    model = row['model']\n",
        "    ax1.scatter(row['aleatoric_uncertainty'], row['epistemic_uncertainty'],\n",
        "               s=300, alpha=0.7, c=MODEL_COLORS[model],\n",
        "               edgecolors='white', linewidth=2, label=model)\n",
        "\n",
        "    # Add text label\n",
        "    ax1.annotate(model.split('-')[0],\n",
        "                (row['aleatoric_uncertainty'], row['epistemic_uncertainty']),\n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "\n",
        "ax1.set_xlabel('Aleatoric Uncertainty', fontsize=11)\n",
        "ax1.set_ylabel('Epistemic Uncertainty', fontsize=11)\n",
        "ax1.set_title('Epistemic vs Aleatoric Uncertainty Trade-off', fontsize=12)\n",
        "ax1.grid(True, alpha=0.3, linewidth=0.5)\n",
        "ax1.legend(frameon=True, fancybox=True, loc='best')\n",
        "\n",
        "# Add diagonal line for equal uncertainty\n",
        "max_val = max(ax1.get_xlim()[1], ax1.get_ylim()[1])\n",
        "ax1.plot([0, max_val], [0, max_val], '--', alpha=0.3, color='gray', label='Equal')\n",
        "\n",
        "\n",
        "# Plot 2: Radar chart of model characteristics\n",
        "ax2 = axes[1]\n",
        "\n",
        "# Prepare data for radar chart\n",
        "categories = ['Calibration', 'Epistemic\\nRatio', 'Volume\\nVariance', 'N Strategies']\n",
        "N = len(categories)\n",
        "\n",
        "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "\n",
        "ax2 = plt.subplot(122, projection='polar')\n",
        "\n",
        "for model in ['GPT2-XL', 'Gemma-2B', 'Mistral-7B-Instruct', 'Llama-3.1-8B-Instruct']:\n",
        "    model_data = uncertainty_decomp_df[uncertainty_decomp_df['model'] == model].iloc[0]\n",
        "    best_cal = calibration_df[calibration_df['model'] == model]['overall_calibration'].max()\n",
        "\n",
        "    values = [\n",
        "        best_cal,\n",
        "        model_data['epistemic_ratio'],\n",
        "        model_data.get('centroid_dispersion', 0) / 5,  # Normalize\n",
        "        model_data['n_strategies'] / 5  # Normalize\n",
        "    ]\n",
        "    values += values[:1]\n",
        "\n",
        "    ax2.plot(angles, values, 'o-', linewidth=2, label=model.split('-')[0],\n",
        "            color=MODEL_COLORS[model], alpha=0.7)\n",
        "    ax2.fill(angles, values, alpha=0.1, color=MODEL_COLORS[model])\n",
        "\n",
        "\n",
        "ax2.set_xticks(angles[:-1])\n",
        "ax2.set_xticklabels(categories, fontsize=10)\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.set_title('Model Characteristics Comparison', fontsize=12, pad=20)\n",
        "ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), frameon=True, fancybox=True)\n",
        "ax2.grid(True, alpha=0.3, linewidth=0.5)\n",
        "\n",
        "\n",
        "plt.suptitle('Uncertainty Components Analysis', fontsize=14, x=0.5, y=1.02)\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_path = os.path.join(CREDAL_DIR, 'epistemic_aleatoric_tradeoff.png')\n",
        "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved epistemic-aleatoric trade-off visualization\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 12. Statistical Analysis\n",
        "\n",
        "# %%\n",
        "print(\"\\nStatistical Analysis...\")\n",
        "\n",
        "# Model size correlation\n",
        "model_sizes = {\n",
        "    'GPT2-XL': 1.5,\n",
        "    'Gemma-2B': 2.0,\n",
        "    'Mistral-7B-Instruct': 7.0,\n",
        "    'Llama-3.1-8B-Instruct': 8.0\n",
        "}\n",
        "\n",
        "# Best calibration per model\n",
        "best_cal_per_model = calibration_df.groupby('model')['overall_calibration'].max()\n",
        "\n",
        "sizes = []\n",
        "calibrations = []\n",
        "for model, size in model_sizes.items():\n",
        "    if model in best_cal_per_model.index:\n",
        "        sizes.append(size)\n",
        "        calibrations.append(best_cal_per_model[model])\n",
        "\n",
        "if len(sizes) > 2:\n",
        "    corr, p_val = stats.spearmanr(sizes, calibrations)\n",
        "    print(f\"\\nModel Size vs Calibration:\")\n",
        "    print(f\"  Spearman ρ = {corr:.3f}, p = {p_val:.3f}\")\n",
        "\n",
        "    # Linear regression\n",
        "    slope, intercept, r_value, p_value, std_err = stats.linregress(sizes, calibrations)\n",
        "    print(f\"  Linear regression: R² = {r_value**2:.3f}, p = {p_value:.3f}\")\n",
        "\n",
        "\n",
        "# Base vs Instruction-tuned\n",
        "base_models = ['GPT2-XL', 'Gemma-2B']\n",
        "instruct_models = ['Mistral-7B-Instruct', 'Llama-3.1-8B-Instruct']\n",
        "\n",
        "base_cal = calibration_df[calibration_df['model'].isin(base_models)]['overall_calibration'].values\n",
        "instruct_cal = calibration_df[calibration_df['model'].isin(instruct_models)]['overall_calibration'].values\n",
        "\n",
        "\n",
        "if len(base_cal) > 0 and len(instruct_cal) > 0:\n",
        "    t_stat, p_val = stats.ttest_ind(base_cal, instruct_cal)\n",
        "    print(f\"\\nBase vs Instruction-tuned Models:\")\n",
        "    print(f\"  Base: μ = {np.mean(base_cal):.3f} ± {np.std(base_cal):.3f}\")\n",
        "    print(f\"  Instruction-tuned: μ = {np.mean(instruct_cal):.3f} ± {np.std(instruct_cal):.3f}\")\n",
        "    print(f\"  t({len(base_cal)+len(instruct_cal)-2}) = {t_stat:.2f}, p = {p_val:.3f}\")\n",
        "    print(f\"  Effect size (Cohen's d): {(np.mean(base_cal) - np.mean(instruct_cal))/np.sqrt((np.var(base_cal) + np.var(instruct_cal))/2):.3f}\")\n",
        "\n",
        "\n",
        "# Epistemic vs Calibration\n",
        "epistemic_by_model = uncertainty_decomp_df.set_index('model')['epistemic_ratio'].to_dict()\n",
        "ep_vals = []\n",
        "cal_vals = []\n",
        "for model in epistemic_by_model.keys():\n",
        "    if model in best_cal_per_model.index:\n",
        "        ep_vals.append(epistemic_by_model[model])\n",
        "        cal_vals.append(best_cal_per_model[model])\n",
        "\n",
        "\n",
        "if len(ep_vals) > 2:\n",
        "    corr_ep, p_val_ep = stats.spearmanr(ep_vals, cal_vals)\n",
        "    print(f\"\\nEpistemic Ratio vs Calibration:\")\n",
        "    print(f\"  Spearman ρ = {corr_ep:.3f}, p = {p_val_ep:.3f}\")\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 13. Generate Publication Tables\n",
        "\n",
        "# %%\n",
        "print(\"\\nGenerating publication-ready tables...\")\n",
        "\n",
        "# Table 1: Human Diversity Baselines\n",
        "print(\"\\n=== Table 1: Human Diversity Baselines ===\")\n",
        "human_stats = []\n",
        "for div_type in ['semantic', 'lexical', 'syntactic']:\n",
        "    if 'human' in diversity_vectors and div_type in diversity_vectors['human']:\n",
        "        vectors = np.array(diversity_vectors['human'][div_type])\n",
        "        if len(vectors) > 0:\n",
        "            # Assuming the first column is the mean metric\n",
        "            human_stats.append({\n",
        "                'Diversity Type': div_type.capitalize(),\n",
        "                'Mean': f\"{np.mean(vectors[:, 0]):.3f}\",\n",
        "                'Std Dev': f\"{np.std(vectors[:, 0]):.3f}\",\n",
        "                'IQR': f\"{np.percentile(vectors[:, 0], 75) - np.percentile(vectors[:, 0], 25):.3f}\",\n",
        "                'N': len(vectors)\n",
        "            })\n",
        "\n",
        "human_table = pd.DataFrame(human_stats)\n",
        "print(human_table.to_string(index=False))\n",
        "human_table.to_csv(os.path.join(CREDAL_DIR, 'table1_human_baselines.csv'), index=False)\n",
        "\n",
        "\n",
        "# Table 2: Model Performance Summary\n",
        "print(\"\\n=== Table 2: Model Performance Summary ===\")\n",
        "model_summary = []\n",
        "for model in ['GPT2-XL', 'Gemma-2B', 'Mistral-7B-Instruct', 'Llama-3.1-8B-Instruct']:\n",
        "    model_df = calibration_df[calibration_df['model'] == model]\n",
        "    decomp_row = uncertainty_decomp_df[uncertainty_decomp_df['model'] == model].iloc[0]\n",
        "\n",
        "    model_summary.append({\n",
        "        'Model': model,\n",
        "        'Size (B)': model_sizes[model],\n",
        "        'Best Calibration': f\"{model_df['overall_calibration'].max():.3f}\",\n",
        "        'Mean Calibration': f\"{model_df['overall_calibration'].mean():.3f}\",\n",
        "        'Epistemic': f\"{decomp_row['epistemic_uncertainty']:.3f}\",\n",
        "        'Aleatoric': f\"{decomp_row['aleatoric_uncertainty']:.3f}\",\n",
        "        'Epistemic Ratio': f\"{decomp_row['epistemic_ratio']:.2%}\"\n",
        "    })\n",
        "\n",
        "model_summary_df = pd.DataFrame(model_summary)\n",
        "print(model_summary_df.to_string(index=False))\n",
        "model_summary_df.to_csv(os.path.join(CREDAL_DIR, 'table2_model_summary.csv'), index=False)\n",
        "\n",
        "\n",
        "# Table 3: Strategy Performance\n",
        "print(\"\\n=== Table 3: Strategy Performance Summary ===\")\n",
        "strategy_summary = calibration_df.groupby('strategy').agg({\n",
        "    'overall_calibration': ['mean', 'std', 'max'],\n",
        "    'overall_distance': 'mean'\n",
        "}).round(3)\n",
        "strategy_summary.columns = ['Mean Cal', 'Std Cal', 'Max Cal', 'Mean Dist']\n",
        "strategy_summary = strategy_summary.sort_values('Mean Cal', ascending=False)\n",
        "print(strategy_summary)\n",
        "strategy_summary.to_csv(os.path.join(CREDAL_DIR, 'table3_strategy_summary.csv'))\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 14. Summary Report\n",
        "\n",
        "# %%\n",
        "# Helper function for JSON serialization\n",
        "def make_json_serializable(obj):\n",
        "    \"\"\"Convert numpy/special types to JSON-serializable formats.\"\"\"\n",
        "    if isinstance(obj, (np.bool_)):\n",
        "        return bool(obj)\n",
        "    elif isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: make_json_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [make_json_serializable(v) for v in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREDAL SET ANALYSIS COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal analysis time: {total_time:.1f} seconds\")\n",
        "\n",
        "\n",
        "print(\"\\nKEY FINDINGS:\")\n",
        "\n",
        "# Best overall configuration\n",
        "best_overall = calibration_df.iloc[0]\n",
        "print(f\"\\n1. Best Overall Configuration:\")\n",
        "print(f\"   Model: {best_overall['source']}\")\n",
        "print(f\"   Calibration: {best_overall['overall_calibration']:.3f}\")\n",
        "print(f\"   Distance: {best_overall['overall_distance']:.3f}\")\n",
        "\n",
        "\n",
        "# Model comparison\n",
        "base_models = ['GPT2-XL', 'Gemma-2B']\n",
        "instruct_models = ['Mistral-7B-Instruct', 'Llama-3.1-8B-Instruct']\n",
        "\n",
        "base_cal = calibration_df[calibration_df['model'].isin(base_models)]['overall_calibration'].values\n",
        "instruct_cal = calibration_df[calibration_df['model'].isin(instruct_models)]['overall_calibration'].values\n",
        "\n",
        "print(f\"\\n2. Model Type Comparison:\")\n",
        "print(f\"   Base models: μ = {np.mean(base_cal):.3f}\")\n",
        "print(f\"   Instruction-tuned: μ = {np.mean(instruct_cal):.3f}\")\n",
        "print(f\"   Finding: Base models show {'better' if np.mean(base_cal) > np.mean(instruct_cal) else 'worse'} calibration\")\n",
        "\n",
        "\n",
        "# Uncertainty decomposition\n",
        "print(f\"\\n3. Uncertainty Decomposition:\")\n",
        "for _, row in uncertainty_decomp_df.iterrows():\n",
        "    print(f\"   {row['model']}:\")\n",
        "    print(f\"     - Epistemic: {row['epistemic_uncertainty']:.3f} ({row['epistemic_ratio']:.1%})\")\n",
        "    print(f\"     - Aleatoric: {row['aleatoric_uncertainty']:.3f}\")\n",
        "\n",
        "\n",
        "# Best strategies\n",
        "print(f\"\\n4. Optimal Strategies:\")\n",
        "print(f\"   Overall best: {calibration_df.groupby('strategy')['overall_calibration'].mean().idxmax()}\")\n",
        "print(f\"   Most consistent: {calibration_df.groupby('strategy')['overall_calibration'].std().idxmin()}\")\n",
        "\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'total_time_seconds': total_time,\n",
        "    'methodology': 'Continuous Credal Sets with Uncertainty Decomposition',\n",
        "    'key_findings': {\n",
        "        'best_configuration': best_overall['source'],\n",
        "        'best_calibration': float(best_overall['overall_calibration']),\n",
        "        'model_size_correlation': float(corr) if 'corr' in locals() else None,\n",
        "        'base_vs_instruct': {\n",
        "            'base_mean': float(np.mean(base_cal)),\n",
        "            'instruct_mean': float(np.mean(instruct_cal)),\n",
        "            'p_value': float(p_val) if 'p_val' in locals() else None\n",
        "        }\n",
        "    },\n",
        "    'uncertainty_decomposition': [\n",
        "        {\n",
        "            'model': row['model'],\n",
        "            'epistemic': float(row['epistemic_uncertainty']),\n",
        "            'aleatoric': float(row['aleatoric_uncertainty']),\n",
        "            'epistemic_ratio': float(row['epistemic_ratio'])\n",
        "        }\n",
        "        for _, row in uncertainty_decomp_df.iterrows()\n",
        "    ],\n",
        "    'files_generated': {\n",
        "        'tables': [\n",
        "            'table1_human_baselines.csv',\n",
        "            'table2_model_summary.csv',\n",
        "            'table3_strategy_summary.csv',\n",
        "            'best_strategy_per_model.csv',\n",
        "            'best_model_per_strategy.csv'\n",
        "        ],\n",
        "        'figures': [\n",
        "            'credal_sets_pca.png',\n",
        "            'uncertainty_analysis.png',\n",
        "            'epistemic_aleatoric_tradeoff.png'\n",
        "        ],\n",
        "        'data': [\n",
        "            'calibration_metrics.csv',\n",
        "            'uncertainty_decomposition.csv'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Clean for JSON serialization\n",
        "summary_clean = make_json_serializable(summary)\n",
        "\n",
        "summary_path = os.path.join(CREDAL_DIR, 'analysis_summary.json')\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(summary_clean, f, indent=2)\n",
        "\n",
        "\n",
        "print(f\"\\nResults saved to: {CREDAL_DIR}\")\n",
        "print(\"\\nANALYSIS COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    }
  ]
}