{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbjUMvYNhOcZ"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# # Uncertainty Quantification Pipeline for Neural Text Generation\n",
        "\n",
        "# This notebook implements uncertainty quantification analysis for neural text generation,\n",
        "# ensuring proper data selection, prompt handling, and multifaceted evaluation metrics.\n",
        "#\n",
        "# **Specifications:**\n",
        "# - 500 prompts with exactly 10 distinct human continuations each\n",
        "# - 4 models × 5 decoding strategies × 10 samples = 1,000 generations per prompt\n",
        "# - Proper prompt removal from all model outputs\n",
        "# - Comprehensive evaluation metrics\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Installation and Setup\n",
        "\n",
        "# %%\n",
        "print(\"Installing required packages...\")\n",
        "print(\"This will take a few minutes...\")\n",
        "\n",
        "# Core packages\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q datasets pandas numpy matplotlib seaborn\n",
        "!pip install -q torch transformers accelerate sentencepiece protobuf\n",
        "!pip install -q sentence-transformers bert-score nltk rouge-score\n",
        "!pip install -q scikit-learn scipy tqdm\n",
        "!pip install -q bitsandbytes  # For 4-bit quantization\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "# Additional packages for advanced metrics\n",
        "!pip install -q textstat  # Readability metrics\n",
        "!pip install -q lexical-diversity  # Vocabulary diversity\n",
        "!pip install -q evaluate  # HuggingFace evaluate library\n",
        "\n",
        "print(\"All packages installed\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Imports and Configuration with NLTK Setup\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Tuple, Optional, Set\n",
        "from scipy import stats\n",
        "import random\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    BitsAndBytesConfig,\n",
        "    set_seed,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList\n",
        ")\n",
        "\n",
        "# Evaluation libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bert_score import score as bert_score\n",
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge_score import rouge_scorer\n",
        "import textstat\n",
        "from lexical_diversity import lex_div as ld\n",
        "import evaluate\n",
        "\n",
        "print(\"All libraries imported\")\n",
        "\n",
        "# NLTK DATA DOWNLOAD WITH ERROR HANDLING\n",
        "print(\"\\nSetting up NLTK resources...\")\n",
        "\n",
        "def setup_nltk_resources():\n",
        "    \"\"\"Download all required NLTK resources with proper error handling\"\"\"\n",
        "    required_resources = [\n",
        "        'punkt',\n",
        "        'punkt_tab',\n",
        "        'averaged_perceptron_tagger',\n",
        "        'averaged_perceptron_tagger_eng',\n",
        "        'wordnet',\n",
        "        'stopwords',\n",
        "        'tagsets',\n",
        "        'maxent_ne_chunker',\n",
        "        'words',\n",
        "        'brown',\n",
        "        'omw-1.4'\n",
        "    ]\n",
        "\n",
        "    failed_resources = []\n",
        "\n",
        "    for resource in required_resources:\n",
        "        try:\n",
        "            nltk.data.find(f'tokenizers/{resource}')\n",
        "            print(f\"   {resource} already present\")\n",
        "        except LookupError:\n",
        "            try:\n",
        "                print(f\"   Downloading {resource}...\")\n",
        "                nltk.download(resource, quiet=True)\n",
        "                print(f\"   {resource} downloaded\")\n",
        "            except Exception as e:\n",
        "                print(f\"   Failed to download {resource}: {str(e)[:50]}\")\n",
        "                failed_resources.append(resource)\n",
        "\n",
        "    # Try alternative downloads for critical resources\n",
        "    if failed_resources:\n",
        "        print(\"\\n   Attempting alternative downloads...\")\n",
        "        try:\n",
        "            nltk.download('all-corpora', quiet=True)\n",
        "            nltk.download('all-nltk', quiet=True)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        still_failed = []\n",
        "        for resource in failed_resources:\n",
        "            try:\n",
        "                nltk.data.find(f'tokenizers/{resource}')\n",
        "            except LookupError:\n",
        "                still_failed.append(resource)\n",
        "\n",
        "        if still_failed:\n",
        "            print(f\"\\n   Some resources unavailable: {still_failed}\")\n",
        "            print(\"   The pipeline will use fallback methods for these.\")\n",
        "\n",
        "    return len(failed_resources) == 0\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk_success = setup_nltk_resources()\n",
        "\n",
        "# Validate critical NLTK functionality\n",
        "print(\"\\nValidating NLTK functionality...\")\n",
        "validation_passed = True\n",
        "\n",
        "try:\n",
        "    test_text = \"This is a test sentence.\"\n",
        "    tokens = nltk.word_tokenize(test_text)\n",
        "    assert len(tokens) > 0, \"Tokenization failed\"\n",
        "    print(\"   Tokenization working\")\n",
        "except Exception as e:\n",
        "    print(f\"   Tokenization issue: {e}\")\n",
        "    validation_passed = False\n",
        "\n",
        "try:\n",
        "    tokens = nltk.word_tokenize(\"The cat sat on the mat.\")\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    assert len(pos_tags) > 0, \"POS tagging failed\"\n",
        "    print(\"   POS tagging working\")\n",
        "except Exception as e:\n",
        "    print(f\"   POS tagging issue: {e}\")\n",
        "    print(\"   Installing fallback...\")\n",
        "    try:\n",
        "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "        nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "    except:\n",
        "        pass\n",
        "    validation_passed = False\n",
        "\n",
        "try:\n",
        "    sentences = nltk.sent_tokenize(\"This is sentence one. This is sentence two.\")\n",
        "    assert len(sentences) == 2, \"Sentence tokenization failed\"\n",
        "    print(\"   Sentence tokenization working\")\n",
        "except Exception as e:\n",
        "    print(f\"   Sentence tokenization issue: {e}\")\n",
        "    validation_passed = False\n",
        "\n",
        "if not validation_passed:\n",
        "    print(\"\\nSome NLTK functions may not work properly. The pipeline will use fallback methods.\")\n",
        "else:\n",
        "    print(\"\\nAll NLTK functions validated successfully\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "set_seed(SEED)\n",
        "\n",
        "# Enable TF32 for A100 optimization\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"   Memory: {gpu_memory:.2f} GB\")\n",
        "\n",
        "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
        "    if 'a100' in gpu_name:\n",
        "        print(\"   A100 detected - enabling optimizations\")\n",
        "        USE_FLASH_ATTENTION = True\n",
        "        BATCH_SIZE_MULTIPLIER = 2\n",
        "    elif 'v100' in gpu_name or 'p100' in gpu_name:\n",
        "        print(\"   V100/P100 detected - using standard settings\")\n",
        "        USE_FLASH_ATTENTION = False\n",
        "        BATCH_SIZE_MULTIPLIER = 1\n",
        "    else:\n",
        "        print(f\"   GPU type: {torch.cuda.get_device_name(0)}\")\n",
        "        USE_FLASH_ATTENTION = False\n",
        "        BATCH_SIZE_MULTIPLIER = 1\n",
        "else:\n",
        "    USE_FLASH_ATTENTION = False\n",
        "    BATCH_SIZE_MULTIPLIER = 0.5\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. HuggingFace Authentication\n",
        "\n",
        "# %%\n",
        "def get_hf_token():\n",
        "    \"\"\"Get HuggingFace token from Colab secrets\"\"\"\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        token = userdata.get('HF_TOKEN')\n",
        "        print(\"Token loaded from Colab secrets\")\n",
        "        return token\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load token from secrets: {e}\")\n",
        "        token = input(\"Please enter your HuggingFace token: \").strip()\n",
        "        if token:\n",
        "            return token\n",
        "        else:\n",
        "            raise ValueError(\"No HuggingFace token provided\")\n",
        "\n",
        "HF_TOKEN = get_hf_token()\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "from huggingface_hub import login\n",
        "try:\n",
        "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "    print(\"Logged in to HuggingFace\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to login to HuggingFace: {e}\")\n",
        "    raise\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Setup Output Directory\n",
        "\n",
        "# %%\n",
        "# Try to use Google Drive for persistent storage\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    SAVE_DIR = '/content/drive/MyDrive/UQ_Analysis'\n",
        "    print(\"Google Drive mounted\")\n",
        "except:\n",
        "    SAVE_DIR = '/content/UQ_Analysis'\n",
        "    print(\"Using local storage (will be lost on session end)\")\n",
        "\n",
        "# Create directory structure\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(SAVE_DIR, 'checkpoints'), exist_ok=True)\n",
        "os.makedirs(os.path.join(SAVE_DIR, 'results'), exist_ok=True)\n",
        "os.makedirs(os.path.join(SAVE_DIR, 'figures'), exist_ok=True)\n",
        "\n",
        "# Test write permissions\n",
        "test_file = os.path.join(SAVE_DIR, 'test.txt')\n",
        "try:\n",
        "    with open(test_file, 'w') as f:\n",
        "        f.write('test')\n",
        "    os.remove(test_file)\n",
        "    print(f\"Save directory ready: {SAVE_DIR}\")\n",
        "except:\n",
        "    print(f\"Cannot write to {SAVE_DIR}\")\n",
        "    SAVE_DIR = './UQ_Analysis'\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "    print(f\"Using fallback directory: {SAVE_DIR}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Load and Select High-Quality Prompts (500 with 10 Unique Stories Each)\n",
        "\n",
        "# %%\n",
        "print(\"\\nLoading WritingPrompts dataset...\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"euclaise/writingprompts\", token=HF_TOKEN)\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "print(f\"Dataset loaded. Total entries: {len(df_train):,}\")\n",
        "\n",
        "# Initialize tokenizer for length analysis\n",
        "from transformers import GPT2Tokenizer\n",
        "length_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "length_tokenizer.pad_token = length_tokenizer.eos_token\n",
        "\n",
        "print(\"\\nFinding prompts with exactly 10 unique human stories...\")\n",
        "\n",
        "# Group stories by prompt and filter for uniqueness\n",
        "prompt_to_stories = defaultdict(list)\n",
        "\n",
        "for idx, row in tqdm(df_train.iterrows(), total=len(df_train), desc=\"Grouping stories\"):\n",
        "    prompt = row['prompt']\n",
        "    story = row['story']\n",
        "\n",
        "    # Quality checks\n",
        "    if not prompt or not story:\n",
        "        continue\n",
        "    if len(prompt) < 20 or len(prompt) > 500:\n",
        "        continue\n",
        "    if len(story) < 200 or len(story) > 10000:\n",
        "        continue\n",
        "\n",
        "    prompt_to_stories[prompt].append(story)\n",
        "\n",
        "print(f\"\\nFound {len(prompt_to_stories)} unique prompts\")\n",
        "\n",
        "# Find prompts with at least 10 UNIQUE stories\n",
        "prompts_with_10_unique = {}\n",
        "story_length_stats = []\n",
        "\n",
        "for prompt, stories in tqdm(prompt_to_stories.items(), desc=\"Finding unique stories\"):\n",
        "    unique_stories = list(set(stories))\n",
        "\n",
        "    if len(unique_stories) >= 10:\n",
        "        story_data = []\n",
        "        for story in unique_stories[:10]:\n",
        "            try:\n",
        "                tokens = length_tokenizer.encode(story, truncation=False, add_special_tokens=False)\n",
        "                story_data.append({\n",
        "                    'text': story,\n",
        "                    'length': len(tokens),\n",
        "                    'hash': hashlib.md5(story.encode()).hexdigest()\n",
        "                })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if len(story_data) == 10:\n",
        "            prompts_with_10_unique[prompt] = story_data\n",
        "            story_length_stats.extend([s['length'] for s in story_data])\n",
        "\n",
        "print(f\"\\nFound {len(prompts_with_10_unique)} prompts with 10+ unique stories\")\n",
        "\n",
        "# Select 500 best prompts (prioritize by diversity of story lengths)\n",
        "def calculate_prompt_quality(prompt, stories):\n",
        "    \"\"\"Score prompts based on story diversity and quality\"\"\"\n",
        "    lengths = [s['length'] for s in stories]\n",
        "    length_diversity = np.std(lengths)\n",
        "    mean_length = np.mean(lengths)\n",
        "\n",
        "    if 100 <= mean_length <= 500:\n",
        "        length_score = 1.0\n",
        "    else:\n",
        "        length_score = 0.5\n",
        "\n",
        "    quality_score = length_diversity * length_score\n",
        "    return quality_score\n",
        "\n",
        "# Score and rank prompts\n",
        "prompt_scores = []\n",
        "for prompt, stories in prompts_with_10_unique.items():\n",
        "    score = calculate_prompt_quality(prompt, stories)\n",
        "    prompt_scores.append((prompt, stories, score))\n",
        "\n",
        "prompt_scores.sort(key=lambda x: x[2], reverse=True)\n",
        "selected_prompts = prompt_scores[:500]\n",
        "\n",
        "print(f\"\\nSelected {len(selected_prompts)} highest quality prompts\")\n",
        "\n",
        "# Prepare final dataset\n",
        "selected_data = {}\n",
        "all_story_lengths = []\n",
        "hash_verification = set()\n",
        "\n",
        "for i, (prompt, stories, score) in enumerate(selected_prompts):\n",
        "    story_hashes = [s['hash'] for s in stories]\n",
        "    assert len(story_hashes) == len(set(story_hashes)), f\"Duplicate stories found in prompt {i}\"\n",
        "\n",
        "    selected_data[str(i)] = {\n",
        "        'prompt': prompt,\n",
        "        'human_stories': [s['text'] for s in stories],\n",
        "        'human_story_lengths': [s['length'] for s in stories],\n",
        "        'human_story_hashes': story_hashes,\n",
        "        'quality_score': score,\n",
        "        'mean_length': np.mean([s['length'] for s in stories]),\n",
        "        'std_length': np.std([s['length'] for s in stories])\n",
        "    }\n",
        "\n",
        "    all_story_lengths.extend([s['length'] for s in stories])\n",
        "    hash_verification.update(story_hashes)\n",
        "\n",
        "# Verify global uniqueness\n",
        "print(f\"\\nVerification:\")\n",
        "print(f\"   Total unique story hashes: {len(hash_verification)}\")\n",
        "print(f\"   Expected (500 * 10): {500 * 10}\")\n",
        "assert len(hash_verification) == 500 * 10, \"Some stories are duplicated across prompts\"\n",
        "\n",
        "# Save selected prompts\n",
        "save_path = os.path.join(SAVE_DIR, 'selected_prompts_verified.json')\n",
        "with open(save_path, 'w') as f:\n",
        "    json.dump(selected_data, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved {len(selected_data)} prompts with verified unique stories\")\n",
        "\n",
        "# Statistics\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"   Total prompts: {len(selected_data)}\")\n",
        "print(f\"   Total unique human stories: {len(selected_data) * 10}\")\n",
        "print(f\"   Mean story length: {np.mean(all_story_lengths):.1f} ± {np.std(all_story_lengths):.1f} tokens\")\n",
        "print(f\"   Median length: {np.median(all_story_lengths):.1f} tokens\")\n",
        "print(f\"   Length range: {min(all_story_lengths)}-{max(all_story_lengths)} tokens\")\n",
        "print(f\"   All stories verified unique\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Model Configurations (4 Models)\n",
        "\n",
        "# %%\n",
        "print(\"\\nConfiguring models...\")\n",
        "\n",
        "# 4-bit quantization configuration for large models\n",
        "bnb_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Model configurations - 4 models\n",
        "MODEL_CONFIGS = {\n",
        "    'GPT2-XL': {\n",
        "        'name': 'gpt2-xl',\n",
        "        'model_class': GPT2LMHeadModel,\n",
        "        'tokenizer_class': GPT2Tokenizer,\n",
        "        'quantization_config': None,\n",
        "        'torch_dtype': torch.float16,\n",
        "        'requires_auth': False,\n",
        "        'batch_size': int(20 * BATCH_SIZE_MULTIPLIER),\n",
        "        'remove_prompt': False,\n",
        "        'max_length': 512\n",
        "    },\n",
        "    'Mistral-7B-Instruct': {\n",
        "        'name': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
        "        'model_class': AutoModelForCausalLM,\n",
        "        'tokenizer_class': AutoTokenizer,\n",
        "        'quantization_config': bnb_config_4bit,\n",
        "        'torch_dtype': torch.float16,\n",
        "        'requires_auth': True,\n",
        "        'batch_size': int(10 * BATCH_SIZE_MULTIPLIER),\n",
        "        'remove_prompt': True,\n",
        "        'max_length': 512\n",
        "    },\n",
        "    'Llama-3.1-8B-Instruct': {\n",
        "        'name': 'meta-llama/Llama-3.1-8B-Instruct',\n",
        "        'model_class': AutoModelForCausalLM,\n",
        "        'tokenizer_class': AutoTokenizer,\n",
        "        'quantization_config': bnb_config_4bit,\n",
        "        'torch_dtype': torch.float16,\n",
        "        'requires_auth': True,\n",
        "        'batch_size': int(8 * BATCH_SIZE_MULTIPLIER),\n",
        "        'remove_prompt': True,\n",
        "        'max_length': 512\n",
        "    },\n",
        "    'Gemma-2B': {\n",
        "        'name': 'google/gemma-2b',\n",
        "        'model_class': AutoModelForCausalLM,\n",
        "        'tokenizer_class': AutoTokenizer,\n",
        "        'quantization_config': None,\n",
        "        'torch_dtype': torch.float16,\n",
        "        'requires_auth': True,\n",
        "        'batch_size': int(25 * BATCH_SIZE_MULTIPLIER),\n",
        "        'remove_prompt': True,\n",
        "        'max_length': 512\n",
        "    }\n",
        "}\n",
        "\n",
        "# Decoding strategies - 5 strategies\n",
        "DECODING_STRATEGIES = {\n",
        "    'temperature_0.7': {\n",
        "        'do_sample': True,\n",
        "        'temperature': 0.7,\n",
        "        'top_k': 0,\n",
        "        'top_p': 1.0,\n",
        "        'repetition_penalty': 1.1\n",
        "    },\n",
        "    'temperature_1.2': {\n",
        "        'do_sample': True,\n",
        "        'temperature': 1.2,\n",
        "        'top_k': 0,\n",
        "        'top_p': 1.0,\n",
        "        'repetition_penalty': 1.1\n",
        "    },\n",
        "    'top_p_0.9': {\n",
        "        'do_sample': True,\n",
        "        'temperature': 1.0,\n",
        "        'top_p': 0.9,\n",
        "        'top_k': 0,\n",
        "        'repetition_penalty': 1.1\n",
        "    },\n",
        "    'top_k_40': {\n",
        "        'do_sample': True,\n",
        "        'temperature': 1.0,\n",
        "        'top_k': 40,\n",
        "        'top_p': 1.0,\n",
        "        'repetition_penalty': 1.1\n",
        "    },\n",
        "    'typical_0.95': {\n",
        "        'do_sample': True,\n",
        "        'temperature': 1.0,\n",
        "        'typical_p': 0.95,\n",
        "        'top_k': 0,\n",
        "        'top_p': 1.0,\n",
        "        'repetition_penalty': 1.1\n",
        "    }\n",
        "}\n",
        "\n",
        "total_generations = len(selected_data) * len(MODEL_CONFIGS) * len(DECODING_STRATEGIES) * 10\n",
        "print(f\"\\nGeneration Plan:\")\n",
        "print(f\"   Prompts: {len(selected_data)}\")\n",
        "print(f\"   Models: {len(MODEL_CONFIGS)} ({', '.join(MODEL_CONFIGS.keys())})\")\n",
        "print(f\"   Strategies: {len(DECODING_STRATEGIES)} ({', '.join(DECODING_STRATEGIES.keys())})\")\n",
        "print(f\"   Samples per config: 10\")\n",
        "print(f\"   Total generations: {total_generations:,}\")\n",
        "\n",
        "estimated_time = total_generations * 0.5 / 60\n",
        "print(f\"   Estimated time: {estimated_time:.1f} minutes ({estimated_time/60:.1f} hours)\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Generation Functions with Proper Prompt Handling\n",
        "\n",
        "# %%\n",
        "def load_model_and_tokenizer(model_config: Dict):\n",
        "    \"\"\"Load model and tokenizer with optimizations\"\"\"\n",
        "    model_name = model_config['name']\n",
        "\n",
        "    try:\n",
        "        print(f\"   Loading {model_name}...\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        tokenizer = model_config['tokenizer_class'].from_pretrained(\n",
        "            model_name,\n",
        "            token=HF_TOKEN if model_config.get('requires_auth', False) else None,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Set padding token\n",
        "        if tokenizer.pad_token is None:\n",
        "            if tokenizer.eos_token:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            else:\n",
        "                tokenizer.pad_token = tokenizer.unk_token\n",
        "\n",
        "        # Model loading arguments\n",
        "        model_kwargs = {\n",
        "            'token': HF_TOKEN if model_config.get('requires_auth', False) else None,\n",
        "            'torch_dtype': model_config['torch_dtype'],\n",
        "            'device_map': \"auto\",\n",
        "            'trust_remote_code': True\n",
        "        }\n",
        "\n",
        "        # Add quantization if specified\n",
        "        if model_config.get('quantization_config'):\n",
        "            model_kwargs['quantization_config'] = model_config['quantization_config']\n",
        "            print(f\"      Using 4-bit quantization\")\n",
        "\n",
        "        # Try Flash Attention 2 for supported models\n",
        "        if USE_FLASH_ATTENTION and 'gpt2' not in model_name.lower():\n",
        "            try:\n",
        "                model_kwargs['use_flash_attention_2'] = True\n",
        "                model = model_config['model_class'].from_pretrained(\n",
        "                    model_name,\n",
        "                    **model_kwargs\n",
        "                )\n",
        "                print(f\"      Loaded with Flash Attention 2\")\n",
        "            except:\n",
        "                del model_kwargs['use_flash_attention_2']\n",
        "                model = model_config['model_class'].from_pretrained(\n",
        "                    model_name,\n",
        "                    **model_kwargs\n",
        "                )\n",
        "                print(f\"      Loaded without Flash Attention\")\n",
        "        else:\n",
        "            model = model_config['model_class'].from_pretrained(\n",
        "                model_name,\n",
        "                **model_kwargs\n",
        "            )\n",
        "            print(f\"      Model loaded successfully\")\n",
        "\n",
        "        # Try torch.compile for speedup (PyTorch 2.0+)\n",
        "        if torch.__version__ >= \"2.0.0\" and 'gpt2' in model_name.lower():\n",
        "            try:\n",
        "                model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "                print(f\"      Model compiled with torch.compile\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Error loading {model_name}: {str(e)[:200]}\")\n",
        "        return None, None\n",
        "\n",
        "def format_prompt_for_generation(prompt: str, model_name: str) -> str:\n",
        "    \"\"\"Format prompt appropriately for each model\"\"\"\n",
        "    prompt = prompt.strip()\n",
        "\n",
        "    if 'instruct' in model_name.lower():\n",
        "        if 'mistral' in model_name.lower():\n",
        "            return f\"[INST] Write a creative story based on this prompt: {prompt} [/INST]\"\n",
        "        elif 'llama' in model_name.lower():\n",
        "            return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWrite a creative story based on this prompt: {prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        elif 'gemma' in model_name.lower():\n",
        "            return f\"<start_of_turn>user\\nWrite a creative story based on this prompt: {prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "    return f\"Story prompt: {prompt}\\n\\nStory:\"\n",
        "\n",
        "def clean_generated_text(generated_text: str, prompt: str, formatted_prompt: str, model_name: str) -> str:\n",
        "    \"\"\"Remove prompt from generated text - critical for proper evaluation\"\"\"\n",
        "\n",
        "    if formatted_prompt in generated_text:\n",
        "        generated_text = generated_text.replace(formatted_prompt, \"\").strip()\n",
        "\n",
        "    patterns_to_remove = [\n",
        "        formatted_prompt,\n",
        "        f\"Write a creative story based on this prompt: {prompt}\",\n",
        "        f\"Story prompt: {prompt}\\n\\nStory:\",\n",
        "        f\"Story prompt: {prompt}\",\n",
        "        prompt\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns_to_remove:\n",
        "        if generated_text.startswith(pattern):\n",
        "            generated_text = generated_text[len(pattern):].strip()\n",
        "\n",
        "    if 'mistral' in model_name.lower():\n",
        "        if '[INST]' in generated_text and '[/INST]' in generated_text:\n",
        "            start = generated_text.find('[/INST]')\n",
        "            if start != -1:\n",
        "                generated_text = generated_text[start + 7:].strip()\n",
        "\n",
        "    elif 'llama' in model_name.lower():\n",
        "        markers = ['<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>']\n",
        "        for marker in markers:\n",
        "            generated_text = generated_text.replace(marker, '')\n",
        "\n",
        "    elif 'gemma' in model_name.lower():\n",
        "        if '<start_of_turn>model' in generated_text:\n",
        "            start = generated_text.find('<start_of_turn>model')\n",
        "            if start != -1:\n",
        "                generated_text = generated_text[start + 21:].strip()\n",
        "        generated_text = generated_text.replace('<end_of_turn>', '').strip()\n",
        "\n",
        "    if generated_text.startswith(prompt):\n",
        "        generated_text = generated_text[len(prompt):].strip()\n",
        "\n",
        "    generated_text = generated_text.lstrip(':').strip()\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def generate_stories_batch(\n",
        "    prompts: List[str],\n",
        "    prompt_ids: List[str],\n",
        "    model,\n",
        "    tokenizer,\n",
        "    model_config: Dict,\n",
        "    decoding_strategy: Dict,\n",
        "    num_samples: int = 10\n",
        ") -> Dict[str, List[str]]:\n",
        "    \"\"\"Generate stories with proper prompt handling and batching\"\"\"\n",
        "\n",
        "    results = {}\n",
        "    model_name = model_config['name']\n",
        "    batch_size = model_config.get('batch_size', 10)\n",
        "    max_length = model_config.get('max_length', 512)\n",
        "    remove_prompt = model_config.get('remove_prompt', True)\n",
        "\n",
        "    for batch_start in tqdm(range(0, len(prompts), batch_size),\n",
        "                            desc=f\"      Generating\", leave=False):\n",
        "        batch_end = min(batch_start + batch_size, len(prompts))\n",
        "        batch_prompts = prompts[batch_start:batch_end]\n",
        "        batch_prompt_ids = prompt_ids[batch_start:batch_end]\n",
        "\n",
        "        formatted_prompts = [format_prompt_for_generation(p, model_name) for p in batch_prompts]\n",
        "\n",
        "        batch_results = {p: [] for p in batch_prompts}\n",
        "\n",
        "        samples_per_pass = min(5, num_samples)\n",
        "        num_passes = (num_samples + samples_per_pass - 1) // samples_per_pass\n",
        "\n",
        "        for pass_idx in range(num_passes):\n",
        "            samples_this_pass = min(samples_per_pass, num_samples - pass_idx * samples_per_pass)\n",
        "\n",
        "            try:\n",
        "                inputs = tokenizer(\n",
        "                    formatted_prompts,\n",
        "                    return_tensors=\"pt\",\n",
        "                    truncation=True,\n",
        "                    max_length=max_length // 2,\n",
        "                    padding=True\n",
        "                )\n",
        "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "                gen_params = {\n",
        "                    **{k: v for k, v in decoding_strategy.items() if k != 'typical_p'},\n",
        "                    'max_new_tokens': max_length // 2,\n",
        "                    'min_new_tokens': 50,\n",
        "                    'num_return_sequences': samples_this_pass,\n",
        "                    'pad_token_id': tokenizer.pad_token_id,\n",
        "                    'eos_token_id': tokenizer.eos_token_id,\n",
        "                    'return_dict_in_generate': False,\n",
        "                }\n",
        "\n",
        "                if 'typical_p' in decoding_strategy:\n",
        "                    try:\n",
        "                        gen_params['typical_p'] = decoding_strategy['typical_p']\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                set_seed(SEED + batch_start + pass_idx * 1000)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                        outputs = model.generate(**inputs, **gen_params)\n",
        "\n",
        "                generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "                for i, original_prompt in enumerate(batch_prompts):\n",
        "                    formatted_prompt = formatted_prompts[i]\n",
        "\n",
        "                    for j in range(samples_this_pass):\n",
        "                        output_idx = i * samples_this_pass + j\n",
        "                        if output_idx < len(generated_texts):\n",
        "                            generated_text = generated_texts[output_idx]\n",
        "\n",
        "                            if remove_prompt:\n",
        "                                story = clean_generated_text(\n",
        "                                    generated_text,\n",
        "                                    original_prompt,\n",
        "                                    formatted_prompt,\n",
        "                                    model_name\n",
        "                                )\n",
        "                            else:\n",
        "                                story = generated_text\n",
        "                                if story.startswith(formatted_prompt):\n",
        "                                    story = story[len(formatted_prompt):].strip()\n",
        "\n",
        "                            if len(story.split()) > 10:\n",
        "                                batch_results[original_prompt].append(story)\n",
        "\n",
        "                del outputs\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            except torch.cuda.OutOfMemoryError:\n",
        "                print(f\"         OOM, reducing batch size\")\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                for i, original_prompt in enumerate(batch_prompts):\n",
        "                    formatted_prompt = formatted_prompts[i]\n",
        "\n",
        "                    for sample_idx in range(samples_this_pass):\n",
        "                        try:\n",
        "                            single_input = tokenizer(\n",
        "                                formatted_prompt,\n",
        "                                return_tensors=\"pt\",\n",
        "                                truncation=True,\n",
        "                                max_length=max_length // 2\n",
        "                            )\n",
        "                            single_input = {k: v.to(model.device) for k, v in single_input.items()}\n",
        "\n",
        "                            with torch.no_grad():\n",
        "                                output = model.generate(\n",
        "                                    **single_input,\n",
        "                                    **{k: v for k, v in gen_params.items() if k != 'num_return_sequences'},\n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "\n",
        "                            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "                            if remove_prompt:\n",
        "                                story = clean_generated_text(\n",
        "                                    generated_text,\n",
        "                                    original_prompt,\n",
        "                                    formatted_prompt,\n",
        "                                    model_name\n",
        "                                )\n",
        "                            else:\n",
        "                                story = generated_text\n",
        "                                if story.startswith(formatted_prompt):\n",
        "                                    story = story[len(formatted_prompt):].strip()\n",
        "\n",
        "                            if len(story.split()) > 10:\n",
        "                                batch_results[original_prompt].append(story)\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"         Error generating sample: {str(e)[:100]}\")\n",
        "                            continue\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"         Generation error: {str(e)[:100]}\")\n",
        "                continue\n",
        "\n",
        "        for prompt in batch_prompts:\n",
        "            stories = batch_results[prompt]\n",
        "\n",
        "            while len(stories) < num_samples:\n",
        "                if stories:\n",
        "                    stories.append(stories[len(stories) % len(stories)])\n",
        "                else:\n",
        "                    stories.append(\"Once upon a time, there was a story that began with this prompt.\")\n",
        "\n",
        "            results[prompt] = stories[:num_samples]\n",
        "\n",
        "    return results\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Comprehensive Evaluation Metrics\n",
        "\n",
        "# %%\n",
        "class ComprehensiveMetrics:\n",
        "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
        "\n",
        "    def __init__(self, device='cuda'):\n",
        "        self.device = device\n",
        "\n",
        "        print(\"   Loading evaluation models...\")\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
        "\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "        try:\n",
        "            self.bleurt = evaluate.load(\"bleurt\", \"BLEURT-20\")\n",
        "            self.has_bleurt = True\n",
        "        except:\n",
        "            self.has_bleurt = False\n",
        "            print(\"      BLEURT not available, skipping\")\n",
        "\n",
        "    def safe_nltk_tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Safe tokenization with fallback\"\"\"\n",
        "        try:\n",
        "            return nltk.word_tokenize(text)\n",
        "        except:\n",
        "            return text.split()\n",
        "\n",
        "    def safe_nltk_sent_tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Safe sentence tokenization with fallback\"\"\"\n",
        "        try:\n",
        "            return nltk.sent_tokenize(text)\n",
        "        except:\n",
        "            sentences = text.split('.')\n",
        "            return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    def safe_pos_tag(self, tokens: List[str]) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Safe POS tagging with fallback\"\"\"\n",
        "        try:\n",
        "            return nltk.pos_tag(tokens)\n",
        "        except:\n",
        "            return [(token, 'NN') for token in tokens]\n",
        "\n",
        "    def calculate_lexical_diversity(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"Calculate various lexical diversity metrics with error handling\"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        try:\n",
        "            combined_text = ' '.join(texts)\n",
        "            tokens = combined_text.split()\n",
        "\n",
        "            metrics['type_token_ratio'] = len(set(tokens)) / len(tokens) if tokens else 0\n",
        "\n",
        "            try:\n",
        "                metrics['mtld'] = ld.mtld(tokens)\n",
        "                metrics['hdd'] = ld.hdd(tokens)\n",
        "            except:\n",
        "                metrics['mtld'] = 0\n",
        "                metrics['hdd'] = 0\n",
        "\n",
        "            metrics['vocab_size'] = len(set(tokens))\n",
        "\n",
        "            word_freq = Counter(tokens)\n",
        "            metrics['hapax_ratio'] = sum(1 for count in word_freq.values() if count == 1) / len(tokens) if tokens else 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      Error in lexical diversity: {str(e)[:50]}\")\n",
        "            metrics = {\n",
        "                'type_token_ratio': 0,\n",
        "                'mtld': 0,\n",
        "                'hdd': 0,\n",
        "                'vocab_size': 0,\n",
        "                'hapax_ratio': 0\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def calculate_syntactic_diversity(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"Calculate syntactic diversity metrics with error handling\"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        try:\n",
        "            all_pos_sequences = []\n",
        "            for text in texts[:10]:\n",
        "                tokens = self.safe_nltk_tokenize(text[:500])\n",
        "                pos_tags = self.safe_pos_tag(tokens)\n",
        "                pos_sequence = ' '.join([tag for _, tag in pos_tags[:50]])\n",
        "                all_pos_sequences.append(pos_sequence)\n",
        "\n",
        "            metrics['unique_pos_patterns'] = len(set(all_pos_sequences))\n",
        "\n",
        "            sentence_lengths = []\n",
        "            for text in texts[:10]:\n",
        "                sentences = self.safe_nltk_sent_tokenize(text)\n",
        "                sentence_lengths.extend([len(s.split()) for s in sentences[:10]])\n",
        "\n",
        "            if sentence_lengths:\n",
        "                metrics['sentence_length_mean'] = np.mean(sentence_lengths)\n",
        "                metrics['sentence_length_std'] = np.std(sentence_lengths)\n",
        "            else:\n",
        "                metrics['sentence_length_mean'] = 0\n",
        "                metrics['sentence_length_std'] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      Error in syntactic diversity: {str(e)[:50]}\")\n",
        "            metrics = {\n",
        "                'unique_pos_patterns': 0,\n",
        "                'sentence_length_mean': 0,\n",
        "                'sentence_length_std': 0\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def calculate_semantic_coherence(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"Calculate semantic coherence and similarity metrics with error handling\"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                embeddings = self.sentence_model.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n",
        "                embeddings_np = embeddings.cpu().numpy()\n",
        "\n",
        "            similarities = cosine_similarity(embeddings_np)\n",
        "\n",
        "            upper_tri = np.triu_indices(len(texts), k=1)\n",
        "            pairwise_sims = similarities[upper_tri]\n",
        "\n",
        "            metrics['semantic_coherence'] = np.mean(pairwise_sims) if len(pairwise_sims) > 0 else 0\n",
        "            metrics['semantic_diversity'] = 1 - metrics['semantic_coherence']\n",
        "            metrics['semantic_coherence_std'] = np.std(pairwise_sims) if len(pairwise_sims) > 0 else 0\n",
        "\n",
        "            if len(pairwise_sims) > 0:\n",
        "                metrics['min_similarity'] = np.min(pairwise_sims)\n",
        "                metrics['max_similarity'] = np.max(pairwise_sims)\n",
        "            else:\n",
        "                metrics['min_similarity'] = 0\n",
        "                metrics['max_similarity'] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      Error in semantic coherence: {str(e)[:50]}\")\n",
        "            metrics = {\n",
        "                'semantic_coherence': 0,\n",
        "                'semantic_diversity': 0,\n",
        "                'semantic_coherence_std': 0,\n",
        "                'min_similarity': 0,\n",
        "                'max_similarity': 0\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def calculate_readability(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"Calculate readability metrics with error handling\"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        try:\n",
        "            readability_scores = []\n",
        "            for text in texts[:10]:\n",
        "                try:\n",
        "                    fre = textstat.flesch_reading_ease(text)\n",
        "                    readability_scores.append(fre)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if readability_scores:\n",
        "                metrics['flesch_reading_ease_mean'] = np.mean(readability_scores)\n",
        "                metrics['flesch_reading_ease_std'] = np.std(readability_scores)\n",
        "            else:\n",
        "                metrics['flesch_reading_ease_mean'] = 0\n",
        "                metrics['flesch_reading_ease_std'] = 0\n",
        "\n",
        "            grade_levels = []\n",
        "            for text in texts[:10]:\n",
        "                try:\n",
        "                    grade = textstat.flesch_kincaid_grade(text)\n",
        "                    grade_levels.append(grade)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if grade_levels:\n",
        "                metrics['grade_level_mean'] = np.mean(grade_levels)\n",
        "                metrics['grade_level_std'] = np.std(grade_levels)\n",
        "            else:\n",
        "                metrics['grade_level_mean'] = 0\n",
        "                metrics['grade_level_std'] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      Error in readability: {str(e)[:50]}\")\n",
        "            metrics = {\n",
        "                'flesch_reading_ease_mean': 0,\n",
        "                'flesch_reading_ease_std': 0,\n",
        "                'grade_level_mean': 0,\n",
        "                'grade_level_std': 0\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def calculate_repetition(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"Calculate repetition metrics with error handling\"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        try:\n",
        "            for n in [2, 3, 4]:\n",
        "                all_ngrams = []\n",
        "                for text in texts[:10]:\n",
        "                    tokens = text.split()[:100]\n",
        "                    ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "                    all_ngrams.extend(ngrams)\n",
        "\n",
        "                if all_ngrams:\n",
        "                    unique_ratio = len(set(all_ngrams)) / len(all_ngrams)\n",
        "                    metrics[f'{n}gram_unique_ratio'] = unique_ratio\n",
        "                else:\n",
        "                    metrics[f'{n}gram_unique_ratio'] = 1.0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      Error in repetition: {str(e)[:50]}\")\n",
        "            metrics = {\n",
        "                '2gram_unique_ratio': 1.0,\n",
        "                '3gram_unique_ratio': 1.0,\n",
        "                '4gram_unique_ratio': 1.0\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def calculate_all_metrics(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"Calculate all metrics for a set of texts with comprehensive error handling\"\"\"\n",
        "        all_metrics = {}\n",
        "\n",
        "        try:\n",
        "            all_metrics['num_texts'] = len(texts)\n",
        "            all_metrics['avg_length_words'] = np.mean([len(t.split()) for t in texts])\n",
        "            all_metrics['std_length_words'] = np.std([len(t.split()) for t in texts])\n",
        "\n",
        "            all_metrics.update(self.calculate_lexical_diversity(texts))\n",
        "            all_metrics.update(self.calculate_syntactic_diversity(texts))\n",
        "            all_metrics.update(self.calculate_semantic_coherence(texts))\n",
        "            all_metrics.update(self.calculate_readability(texts))\n",
        "            all_metrics.update(self.calculate_repetition(texts))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      Error in calculate_all_metrics: {str(e)[:50]}\")\n",
        "            all_metrics = {\n",
        "                'num_texts': len(texts),\n",
        "                'avg_length_words': 0,\n",
        "                'std_length_words': 0,\n",
        "                'semantic_diversity': 0\n",
        "            }\n",
        "\n",
        "        return all_metrics\n",
        "\n",
        "# Initialize metrics calculator\n",
        "print(\"\\nInitializing comprehensive metrics calculator...\")\n",
        "metrics_calculator = ComprehensiveMetrics(device=device)\n",
        "print(\"Metrics calculator ready\")\n",
        "\n",
        "# Test metrics calculator\n",
        "print(\"\\nTesting metrics calculator...\")\n",
        "test_texts = [\"This is a test sentence.\", \"Another test sentence here.\"]\n",
        "test_metrics = metrics_calculator.calculate_all_metrics(test_texts)\n",
        "if test_metrics['num_texts'] == 2:\n",
        "    print(\"Metrics calculator test passed\")\n",
        "else:\n",
        "    print(\"Metrics calculator may have issues, but continuing...\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Main Generation Pipeline (Metrics Calculated After Generation)\n",
        "\n",
        "# %%\n",
        "print(\"\\nStarting main generation pipeline...\")\n",
        "print(f\"   This will take several hours. Starting at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "generation_metadata = []\n",
        "\n",
        "# Track start time\n",
        "pipeline_start_time = time.time()\n",
        "\n",
        "# Process each model\n",
        "for model_idx, (model_key, model_config) in enumerate(MODEL_CONFIGS.items()):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[{model_idx+1}/{len(MODEL_CONFIGS)}] Processing {model_key}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_start = time.time()\n",
        "\n",
        "    # Load model\n",
        "    model, tokenizer = load_model_and_tokenizer(model_config)\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"   Could not load {model_key}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Process each decoding strategy\n",
        "    for strategy_idx, (strategy_name, strategy_params) in enumerate(DECODING_STRATEGIES.items()):\n",
        "        print(f\"\\n   Strategy [{strategy_idx+1}/{len(DECODING_STRATEGIES)}]: {strategy_name}\")\n",
        "        source_key = f\"{model_key}_{strategy_name}\"\n",
        "        strategy_start = time.time()\n",
        "\n",
        "        # Prepare prompts\n",
        "        prompt_ids = list(selected_data.keys())\n",
        "        prompts = [selected_data[pid]['prompt'] for pid in prompt_ids]\n",
        "\n",
        "        # Generate stories in batches\n",
        "        print(f\"      Generating {len(prompts)} prompts × 10 samples = {len(prompts)*10} stories\")\n",
        "        generated_stories = generate_stories_batch(\n",
        "            prompts,\n",
        "            prompt_ids,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            model_config,\n",
        "            strategy_params,\n",
        "            num_samples=10\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        stories_generated = 0\n",
        "        for prompt_id, prompt in zip(prompt_ids, prompts):\n",
        "            stories = generated_stories.get(prompt, [])\n",
        "\n",
        "            assert len(stories) == 10, f\"Expected 10 stories, got {len(stories)} for prompt {prompt_id}\"\n",
        "\n",
        "            for story_idx, story in enumerate(stories):\n",
        "                story_length = len(tokenizer.encode(story, add_special_tokens=False))\n",
        "\n",
        "                result = {\n",
        "                    'prompt_id': prompt_id,\n",
        "                    'prompt': prompt,\n",
        "                    'source': source_key,\n",
        "                    'model': model_key,\n",
        "                    'strategy': strategy_name,\n",
        "                    'story_index': story_idx,\n",
        "                    'story': story,\n",
        "                    'story_length_tokens': story_length,\n",
        "                    'story_length_words': len(story.split()),\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                all_results.append(result)\n",
        "                stories_generated += 1\n",
        "\n",
        "        # Store basic metadata\n",
        "        generation_metadata.append({\n",
        "            'source': source_key,\n",
        "            'model': model_key,\n",
        "            'strategy': strategy_name,\n",
        "            'stories_generated': stories_generated,\n",
        "            'generation_time': time.time() - strategy_start\n",
        "        })\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_df = pd.DataFrame(all_results)\n",
        "        checkpoint_path = os.path.join(SAVE_DIR, 'checkpoints', f'checkpoint_{source_key}.parquet')\n",
        "        checkpoint_df.to_parquet(checkpoint_path)\n",
        "        print(f\"      Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "        strategy_time = time.time() - strategy_start\n",
        "        print(f\"      Completed in {strategy_time/60:.1f} minutes\")\n",
        "        print(f\"      Generated {stories_generated} stories\")\n",
        "\n",
        "    # Model completion\n",
        "    model_time = time.time() - model_start\n",
        "    print(f\"\\n   {model_key} completed in {model_time/60:.1f} minutes\")\n",
        "\n",
        "    # Clear model from memory\n",
        "    del model\n",
        "    del tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Estimate remaining time\n",
        "    elapsed = time.time() - pipeline_start_time\n",
        "    models_done = model_idx + 1\n",
        "    if models_done < len(MODEL_CONFIGS):\n",
        "        avg_per_model = elapsed / models_done\n",
        "        remaining = avg_per_model * (len(MODEL_CONFIGS) - models_done)\n",
        "        print(f\"   Estimated remaining: {remaining/60:.1f} minutes ({remaining/3600:.1f} hours)\")\n",
        "\n",
        "print(f\"\\nAll models completed. Total generation time: {(time.time() - pipeline_start_time)/60:.1f} minutes\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Add Human Stories to Results\n",
        "\n",
        "# %%\n",
        "print(\"\\nAdding human stories to results...\")\n",
        "\n",
        "human_stories_added = 0\n",
        "for prompt_id, prompt_data in selected_data.items():\n",
        "    prompt_text = prompt_data['prompt']\n",
        "    human_stories = prompt_data['human_stories']\n",
        "    human_lengths = prompt_data['human_story_lengths']\n",
        "\n",
        "    for story_idx, (story, length) in enumerate(zip(human_stories, human_lengths)):\n",
        "        result = {\n",
        "            'prompt_id': prompt_id,\n",
        "            'prompt': prompt_text,\n",
        "            'source': 'human',\n",
        "            'model': 'human',\n",
        "            'strategy': 'human',\n",
        "            'story_index': story_idx,\n",
        "            'story': story,\n",
        "            'story_length_tokens': length,\n",
        "            'story_length_words': len(story.split()),\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        all_results.append(result)\n",
        "        human_stories_added += 1\n",
        "\n",
        "print(f\"Added {human_stories_added} human stories\")\n",
        "print(f\"   Total stories in dataset: {len(all_results)}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Save complete results\n",
        "results_path = os.path.join(SAVE_DIR, 'results', 'all_stories_complete.parquet')\n",
        "results_df.to_parquet(results_path)\n",
        "print(f\"Saved complete results to: {results_path}\")\n",
        "\n",
        "# Also save as CSV for easier inspection\n",
        "csv_path = os.path.join(SAVE_DIR, 'results', 'all_stories_complete.csv')\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "print(f\"Saved CSV version to: {csv_path}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 11. Calculate Comprehensive Metrics for All Sources\n",
        "\n",
        "# %%\n",
        "print(\"\\nCalculating comprehensive metrics for all sources...\")\n",
        "\n",
        "metrics_results = []\n",
        "\n",
        "for source in tqdm(results_df['source'].unique(), desc=\"Calculating metrics\"):\n",
        "    source_stories = results_df[results_df['source'] == source]['story'].tolist()\n",
        "\n",
        "    sample_stories = source_stories[:100]\n",
        "\n",
        "    print(f\"   Processing {source}: {len(sample_stories)} stories\")\n",
        "\n",
        "    try:\n",
        "        source_metrics = metrics_calculator.calculate_all_metrics(sample_stories)\n",
        "        source_metrics['source'] = source\n",
        "        source_metrics['model'] = source.split('_')[0] if '_' in source else source\n",
        "        source_metrics['strategy'] = '_'.join(source.split('_')[1:]) if '_' in source else 'human'\n",
        "        metrics_results.append(source_metrics)\n",
        "    except Exception as e:\n",
        "        print(f\"   Error calculating metrics for {source}: {str(e)[:100]}\")\n",
        "        metrics_results.append({\n",
        "            'source': source,\n",
        "            'model': source.split('_')[0] if '_' in source else source,\n",
        "            'strategy': '_'.join(source.split('_')[1:]) if '_' in source else 'human',\n",
        "            'semantic_diversity': 0,\n",
        "            'num_texts': len(sample_stories)\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_results)\n",
        "\n",
        "# Save metrics\n",
        "metrics_path = os.path.join(SAVE_DIR, 'results', 'comprehensive_metrics.csv')\n",
        "metrics_df.to_csv(metrics_path, index=False)\n",
        "print(f\"Saved comprehensive metrics to: {metrics_path}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 12. Calculate Uncertainty Metrics\n",
        "\n",
        "# %%\n",
        "print(\"\\nCalculating uncertainty metrics...\")\n",
        "\n",
        "def calculate_diversity_for_source_safe(stories: List[str], metric_type: str = 'semantic') -> float:\n",
        "    \"\"\"Calculate diversity for a set of stories with error handling\"\"\"\n",
        "    if len(stories) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        if metric_type == 'semantic':\n",
        "            with torch.no_grad():\n",
        "                embeddings = metrics_calculator.sentence_model.encode(\n",
        "                    stories[:50],\n",
        "                    convert_to_tensor=True,\n",
        "                    show_progress_bar=False\n",
        "                )\n",
        "                embeddings_np = embeddings.cpu().numpy()\n",
        "                similarities = cosine_similarity(embeddings_np)\n",
        "                upper_tri = np.triu_indices(len(embeddings_np), k=1)\n",
        "                diversity = 1 - np.mean(similarities[upper_tri])\n",
        "                return diversity\n",
        "\n",
        "        elif metric_type == 'lexical':\n",
        "            vocabs = [set(story.lower().split()[:200]) for story in stories[:50]]\n",
        "            overlaps = []\n",
        "            for i in range(len(vocabs)):\n",
        "                for j in range(i+1, len(vocabs)):\n",
        "                    intersection = len(vocabs[i] & vocabs[j])\n",
        "                    union = len(vocabs[i] | vocabs[j])\n",
        "                    overlap = intersection / union if union > 0 else 0\n",
        "                    overlaps.append(overlap)\n",
        "            diversity = 1 - np.mean(overlaps) if overlaps else 0\n",
        "            return diversity\n",
        "\n",
        "        elif metric_type == 'syntactic':\n",
        "            pos_patterns = []\n",
        "            for story in stories[:20]:\n",
        "                try:\n",
        "                    tokens = metrics_calculator.safe_nltk_tokenize(story[:500])\n",
        "                    pos_tags = metrics_calculator.safe_pos_tag(tokens[:50])\n",
        "                    pattern = ' '.join([tag for _, tag in pos_tags[:30]])\n",
        "                    pos_patterns.append(pattern)\n",
        "                except:\n",
        "                    pos_patterns.append(\"\")\n",
        "\n",
        "            unique_patterns = len(set(pos_patterns))\n",
        "            diversity = unique_patterns / len(pos_patterns) if pos_patterns else 0\n",
        "            return diversity\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"      Error in diversity calculation ({metric_type}): {str(e)[:50]}\")\n",
        "        return 0.0\n",
        "\n",
        "    return 0.0\n",
        "\n",
        "# Calculate uncertainty for each prompt and source\n",
        "uncertainty_results = []\n",
        "\n",
        "for prompt_id in tqdm(selected_data.keys(), desc=\"Calculating uncertainties\"):\n",
        "    prompt_data = results_df[results_df['prompt_id'] == prompt_id]\n",
        "\n",
        "    human_stories = prompt_data[prompt_data['source'] == 'human']['story'].tolist()\n",
        "\n",
        "    if len(human_stories) != 10:\n",
        "        continue\n",
        "\n",
        "    human_semantic_div = calculate_diversity_for_source_safe(human_stories, 'semantic')\n",
        "    human_lexical_div = calculate_diversity_for_source_safe(human_stories, 'lexical')\n",
        "    human_syntactic_div = calculate_diversity_for_source_safe(human_stories, 'syntactic')\n",
        "\n",
        "    for source in prompt_data['source'].unique():\n",
        "        if source == 'human':\n",
        "            continue\n",
        "\n",
        "        model_stories = prompt_data[prompt_data['source'] == source]['story'].tolist()\n",
        "\n",
        "        if len(model_stories) != 10:\n",
        "            continue\n",
        "\n",
        "        model_semantic_div = calculate_diversity_for_source_safe(model_stories, 'semantic')\n",
        "        model_lexical_div = calculate_diversity_for_source_safe(model_stories, 'lexical')\n",
        "        model_syntactic_div = calculate_diversity_for_source_safe(model_stories, 'syntactic')\n",
        "\n",
        "        combined_stories = human_stories[:5] + model_stories[:5]\n",
        "        cross_semantic_div = calculate_diversity_for_source_safe(combined_stories, 'semantic')\n",
        "\n",
        "        semantic_calibration = abs(model_semantic_div - human_semantic_div)\n",
        "        lexical_calibration = abs(model_lexical_div - human_lexical_div)\n",
        "        syntactic_calibration = abs(model_syntactic_div - human_syntactic_div)\n",
        "\n",
        "        uncertainty_results.append({\n",
        "            'prompt_id': prompt_id,\n",
        "            'source': source,\n",
        "            'model': source.split('_')[0],\n",
        "            'strategy': '_'.join(source.split('_')[1:]),\n",
        "            'human_semantic_diversity': human_semantic_div,\n",
        "            'human_lexical_diversity': human_lexical_div,\n",
        "            'human_syntactic_diversity': human_syntactic_div,\n",
        "            'model_semantic_diversity': model_semantic_div,\n",
        "            'model_lexical_diversity': model_lexical_div,\n",
        "            'model_syntactic_diversity': model_syntactic_div,\n",
        "            'cross_semantic_diversity': cross_semantic_div,\n",
        "            'semantic_calibration_error': semantic_calibration,\n",
        "            'lexical_calibration_error': lexical_calibration,\n",
        "            'syntactic_calibration_error': syntactic_calibration,\n",
        "            'overall_calibration_error': (semantic_calibration + lexical_calibration + syntactic_calibration) / 3\n",
        "        })\n",
        "\n",
        "uncertainty_df = pd.DataFrame(uncertainty_results)\n",
        "\n",
        "# Save uncertainty analysis\n",
        "uncertainty_path = os.path.join(SAVE_DIR, 'results', 'uncertainty_analysis.csv')\n",
        "uncertainty_df.to_csv(uncertainty_path, index=False)\n",
        "print(f\"Saved uncertainty analysis to: {uncertainty_path}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 13. Statistical Analysis and Key Findings\n",
        "\n",
        "# %%\n",
        "print(\"\\nSTATISTICAL ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Aggregate by model and strategy\n",
        "model_performance = uncertainty_df.groupby(['model', 'strategy']).agg({\n",
        "    'semantic_calibration_error': ['mean', 'std'],\n",
        "    'lexical_calibration_error': ['mean', 'std'],\n",
        "    'syntactic_calibration_error': ['mean', 'std'],\n",
        "    'overall_calibration_error': ['mean', 'std'],\n",
        "    'model_semantic_diversity': ['mean', 'std'],\n",
        "    'model_lexical_diversity': ['mean', 'std'],\n",
        "    'model_syntactic_diversity': ['mean', 'std']\n",
        "}).round(4)\n",
        "\n",
        "print(\"\\nMODEL PERFORMANCE RANKING (by overall calibration error):\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Best configurations\n",
        "best_configs = uncertainty_df.groupby('source')['overall_calibration_error'].mean().sort_values()\n",
        "for rank, (source, error) in enumerate(best_configs.head(10).items(), 1):\n",
        "    model = source.split('_')[0]\n",
        "    strategy = '_'.join(source.split('_')[1:])\n",
        "    print(f\"{rank:2d}. {model:20s} + {strategy:15s} : {error:.4f}\")\n",
        "\n",
        "# Human baseline\n",
        "human_data = results_df[results_df['source'] == 'human']\n",
        "human_diversity_stats = {\n",
        "    'semantic': [],\n",
        "    'lexical': [],\n",
        "    'syntactic': []\n",
        "}\n",
        "\n",
        "for prompt_id in selected_data.keys():\n",
        "    human_stories = human_data[human_data['prompt_id'] == prompt_id]['story'].tolist()\n",
        "    if len(human_stories) == 10:\n",
        "        human_diversity_stats['semantic'].append(calculate_diversity_for_source_safe(human_stories, 'semantic'))\n",
        "        human_diversity_stats['lexical'].append(calculate_diversity_for_source_safe(human_stories, 'lexical'))\n",
        "        human_diversity_stats['syntactic'].append(calculate_diversity_for_source_safe(human_stories, 'syntactic'))\n",
        "\n",
        "print(f\"\\nHUMAN BASELINE DIVERSITY:\")\n",
        "print(f\"   Semantic:  {np.mean(human_diversity_stats['semantic']):.4f} ± {np.std(human_diversity_stats['semantic']):.4f}\")\n",
        "print(f\"   Lexical:   {np.mean(human_diversity_stats['lexical']):.4f} ± {np.std(human_diversity_stats['lexical']):.4f}\")\n",
        "print(f\"   Syntactic: {np.mean(human_diversity_stats['syntactic']):.4f} ± {np.std(human_diversity_stats['syntactic']):.4f}\")\n",
        "\n",
        "# Model comparison\n",
        "print(f\"\\nMODEL COMPARISON:\")\n",
        "for model_name in MODEL_CONFIGS.keys():\n",
        "    model_data = uncertainty_df[uncertainty_df['model'] == model_name]\n",
        "    if len(model_data) > 0:\n",
        "        print(f\"\\n   {model_name}:\")\n",
        "        print(f\"      Semantic diversity:  {model_data['model_semantic_diversity'].mean():.4f}\")\n",
        "        print(f\"      Calibration error:   {model_data['overall_calibration_error'].mean():.4f}\")\n",
        "\n",
        "        model_stories_all = results_df[results_df['model'] == model_name]\n",
        "        if len(model_stories_all) > 0:\n",
        "            model_divs = model_data['model_semantic_diversity'].values[:100]\n",
        "            human_divs = human_diversity_stats['semantic'][:100]\n",
        "\n",
        "            if len(model_divs) > 0 and len(human_divs) > 0:\n",
        "                t_stat, p_value = stats.ttest_ind(model_divs, human_divs)\n",
        "                print(f\"      vs Human (p-value): {p_value:.6f} {'(ns)' if p_value > 0.05 else '(*)'}\")\n",
        "\n"
      ]
    }
  ]
}